{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1efef0-2377-46f2-b1c0-08cf58ce5bba",
   "metadata": {},
   "source": [
    "## Import Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe87340-5dc4-4da9-afb6-ef7ec89d93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "#!pip install networkx\n",
    "#!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46d3d05e-0dae-403d-becd-40c3b684c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import xgboost as xgb\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed\n",
    "seed = 2024\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b4fa1-e2f6-4703-bf80-8dbd7314947c",
   "metadata": {},
   "source": [
    "## Load MoneyPuck raw shot data and standardize structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ef5d7c-c321-4c24-bf91-54f1c3b59a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the ten csv's downloaded from Moneypuck (https://moneypuck.com/data.htm)\n",
    "#shots1415 = pd.read_csv(\"shots_2014_2015.csv\")\n",
    "#shots1516 = pd.read_csv(\"shots_2015_2016.csv\")\n",
    "#shots1617 = pd.read_csv(\"shots_2016_2017.csv\")\n",
    "#shots1718 = pd.read_csv(\"shots_2017_2018.csv\")\n",
    "#shots1819 = pd.read_csv(\"shots_2018_2019.csv\")\n",
    "shots1920 = pd.read_csv(\"shots_2019_2020.csv\")\n",
    "shots2021 = pd.read_csv(\"shots_2020_2021.csv\")\n",
    "shots2122 = pd.read_csv(\"shots_2021_2022.csv\")\n",
    "shots2223 = pd.read_csv(\"shots_2022_2023.csv\")\n",
    "shots2324 = pd.read_csv(\"shots_2023_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "756efc4e-305a-461c-a24a-4e09ab17a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104172, 124)\n",
      "(78611, 137)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['gameOver',\n",
       " 'homeTeamScore',\n",
       " 'homeWinProbability',\n",
       " 'penaltyLength',\n",
       " 'playoffGame',\n",
       " 'roadTeamCode',\n",
       " 'roadTeamScore',\n",
       " 'shotGoalProbability',\n",
       " 'shotPlayContinued',\n",
       " 'timeBetweenEvents',\n",
       " 'timeLeft',\n",
       " 'wentToOT',\n",
       " 'wentToShootout']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show differing number of columns\n",
    "print(shots1920.shape)\n",
    "print(shots2021.shape)\n",
    "shots2021.columns.difference(shots1920.columns).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de8f35c-2a98-4bca-b975-772f1aecf91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78611, 124)\n",
      "(104172, 124)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns from 2020-21 season that don't exist in the previous season\n",
    "shots2021.drop(shots2021.columns.difference(shots1920.columns).tolist(), axis=1, inplace=True)\n",
    "print(shots2021.shape)\n",
    "print(shots1920.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebfd3312-8cc9-4712-9661-b44fbe0006ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in seperate dataframes: 548752\n",
      "num rows in combined dataframe:  548752\n"
     ]
    }
   ],
   "source": [
    "# concat all the shots dataframes together to simplify usage\n",
    "pdList = []\n",
    "pdList.extend(value for name, value in locals().items() if name.startswith('shots'))\n",
    "print(f\"num rows in seperate dataframes: {sum([len(x) for x in pdList])}\")\n",
    "_df = pd.concat(pdList, ignore_index=True)\n",
    "print(f\"num rows in combined dataframe:  {len(_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aa9608-5174-49b2-b231-e858a03d469c",
   "metadata": {},
   "source": [
    "## Cleanse raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73a14dff-4908-4431-9cac-325f3e88675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure team abbreviations are consistent (ex. L.A -> LAK)\n",
    "for (old, new) in [\n",
    "    ('L.A', 'LAK'),\n",
    "    ('N.J', 'NJD'),\n",
    "    ('S.J', 'SJS'),\n",
    "    ('T.B', 'TBL')\n",
    "]:\n",
    "    _df.loc[_df.homeTeamCode == old, \"homeTeamCode\"] = new\n",
    "    _df.loc[_df.awayTeamCode == old, \"awayTeamCode\"] = new\n",
    "    _df.loc[_df.teamCode == old, \"teamCode\"] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66752b69-4cd4-4e4e-83ea-20d19a94a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update \"Sebastian Aho\" -> \"Sebastian Aho NYI\"\n",
    "_df.loc[(_df.shooterName == \"Sebastian Aho\") & (_df.teamCode == \"NYI\"), \"shooterName\"] = \"Sebastian Aho NYI\"\n",
    "\n",
    "# Update \"Matt Murray\" -> \"Matt Murray DAL\" (only played 2022-23 and 2023-24, have to go game by game because nothing on goalie side indicates which it is)\n",
    "_df.loc[(_df.goalieNameForShot == \"Matt Murray\") & (_df.game_id == 20979) & (_df.season == 2022), \"goalieNameForShot\"] = \"Matt Murray DAL\"\n",
    "_df.loc[(_df.goalieNameForShot == \"Matt Murray\") & (_df.game_id == 21073) & (_df.season == 2022), \"goalieNameForShot\"] = \"Matt Murray DAL\"\n",
    "_df.loc[(_df.goalieNameForShot == \"Matt Murray\") & (_df.game_id == 21161) & (_df.season == 2022), \"goalieNameForShot\"] = \"Matt Murray DAL\"\n",
    "_df.loc[(_df.goalieNameForShot == \"Matt Murray\") & (_df.season == 2023), \"goalieNameForShot\"] = \"Matt Murray DAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a24b9c-b19f-4889-98d5-6ed7ef3225f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 393996\n"
     ]
    }
   ],
   "source": [
    "# Only keep data about shots and goals\n",
    "_df = _df[_df.event != \"MISS\"]\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11bcb717-7d32-4e82-8e51-d452c7f12e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 393996\n"
     ]
    }
   ],
   "source": [
    "# Remove columns that are unneeded, expected values and info about post shot activity\n",
    "_df = _df.drop([\n",
    "    \"shotID\",\n",
    "    \"goalieIdForShot\",\n",
    "    \"id\",\n",
    "    \"playerNumThatDidEvent\",\n",
    "    \"event\",\n",
    "    \"playerNumThatDidLastEvent\",\n",
    "    \"shooterPlayerId\",\n",
    "    \"xFroze\",\n",
    "    \"xPlayContinuedInZone\",\n",
    "    \"xPlayContinuedOutsideZone\",\n",
    "    \"xPlayStopped\",\n",
    "    \"xRebound\",\n",
    "    \"xShotWasOnGoal\", \n",
    "    \"homeTeamWon\",\n",
    "    \"shotAnglePlusRebound\",\n",
    "    \"shotGeneratedRebound\", \n",
    "    \"shotGoalieFroze\", \n",
    "    \"shotPlayContinuedInZone\",\n",
    "    \"shotPlayContinuedOutsideZone\",\n",
    "    \"shotPlayStopped\",\n",
    "    \"timeUntilNextEvent\"\n",
    "], axis=1, errors='ignore')\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee3d640-c480-43ca-9107-132497c7b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 391465\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing data in max/min time on ice rows (MoneyPuck says that this is denoted when a max is set to 0 and a min set to 999)\n",
    "times_to_exclude_min = [999]\n",
    "times_to_exclude_max = [0]\n",
    "_df = _df[\n",
    "    (~_df.defendingTeamMaxTimeOnIce.isin(times_to_exclude_max)) &\n",
    "    (~_df.defendingTeamMaxTimeOnIceOfDefencemen.isin(times_to_exclude_max)) & \n",
    "    (~_df.defendingTeamMaxTimeOnIceOfDefencemenSinceFaceoff.isin(times_to_exclude_max)) &\n",
    "    (~_df.defendingTeamMaxTimeOnIceOfForwards.isin(times_to_exclude_max)) & \n",
    "    (~_df.defendingTeamMaxTimeOnIceOfForwardsSinceFaceoff.isin(times_to_exclude_max)) & \n",
    "    (~_df.defendingTeamMaxTimeOnIceSinceFaceoff.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIce.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIceOfDefencemen.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIceOfDefencemenSinceFaceoff.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIceOfForwards.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIceOfForwardsSinceFaceoff.isin(times_to_exclude_max)) & \n",
    "    (~_df.shootingTeamMaxTimeOnIceSinceFaceoff.isin(times_to_exclude_max)) &\n",
    "    (~_df.defendingTeamMinTimeOnIce.isin(times_to_exclude_min)) &\n",
    "    (~_df.defendingTeamMinTimeOnIceOfDefencemen.isin(times_to_exclude_min)) & \n",
    "    (~_df.defendingTeamMinTimeOnIceOfDefencemenSinceFaceoff.isin(times_to_exclude_min)) &\n",
    "    (~_df.defendingTeamMinTimeOnIceOfForwards.isin(times_to_exclude_min)) & \n",
    "    (~_df.defendingTeamMinTimeOnIceOfForwardsSinceFaceoff.isin(times_to_exclude_min)) & \n",
    "    (~_df.defendingTeamMinTimeOnIceSinceFaceoff.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIce.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIceOfDefencemen.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIceOfDefencemenSinceFaceoff.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIceOfForwards.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIceOfForwardsSinceFaceoff.isin(times_to_exclude_min)) & \n",
    "    (~_df.shootingTeamMinTimeOnIceSinceFaceoff.isin(times_to_exclude_min))\n",
    "]\n",
    "\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a3dc561-23d4-45e8-ad0f-1fe13a4fbaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 387983\n"
     ]
    }
   ],
   "source": [
    "# deal with NAs\n",
    "for c in [\"shotType\", \"playerPositionThatDidEvent\", \"shooterName\", \"shooterLeftRight\"]:\n",
    "    _df = _df[_df[c].notna()]\n",
    "# Fill nan's when there's a goal on an empty net\n",
    "_df.goalieNameForShot = _df.goalieNameForShot.fillna(\"Empty Net\")\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92dc5d62-05e5-4826-be85-f3c6bab7eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 387952\n"
     ]
    }
   ],
   "source": [
    "# Remove shots taken by other goaltender\n",
    "_df = _df[_df[\"playerPositionThatDidEvent\"] != \"G\"]\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e95cc80-69be-41de-81e7-a725e6ea4111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 387932\n"
     ]
    }
   ],
   "source": [
    "# Remove shots with too few or too many skaters on the ice\n",
    "# There should be between 6 and 11 skaters (possibly 12, but both teams won't have goalies pulled simultaneously)\n",
    "player_num_cols = [\"shootingTeamForwardsOnIce\", \"shootingTeamDefencemenOnIce\", \"defendingTeamForwardsOnIce\", \"defendingTeamDefencemenOnIce\"]\n",
    "_df = _df[_df[player_num_cols].sum(axis=1).between(6, 11)]\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7517c09-5f48-4a5e-8cd6-6abbc57cfc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleansed data\n",
    "_df.to_csv(\"cleansed_shot_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0a385-9e4d-4be4-83f8-923e26f67032",
   "metadata": {},
   "source": [
    "## Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54f4e583-a066-4d17-adad-e50206515d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in combined dataframe: 387932\n",
      "num cols in combined dataframe: 103\n"
     ]
    }
   ],
   "source": [
    "# Import cleansed data\n",
    "_df = pd.read_csv(\"cleansed_shot_data.csv\", )\n",
    "print(f\"num rows in combined dataframe: {len(_df)}\")\n",
    "print(f\"num cols in combined dataframe: {len(_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e90b9195-fa72-4ee3-b6de-b386551c19b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding\n",
    "# https://maxhalford.github.io/blog/target-encoding/\n",
    "def calc_smooth_mean(df, by, on, m):\n",
    "    # Compute the global mean\n",
    "    mean = df[on].mean()\n",
    "    \n",
    "    # Compute the number of values and the mean of each group\n",
    "    agg = df.groupby(by)[on].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means = agg['mean']\n",
    "    \n",
    "    # Compute the 'smoothed' mean\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "    \n",
    "    # Replace each value by the according smoothed mean\n",
    "    return df[by].map(smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f51d136d-56b1-4458-8042-23cc872b4e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in encoded dataframe: 387932\n",
      "num cols in encoded dataframe: 116\n"
     ]
    }
   ],
   "source": [
    "# Create the one hot encoding for low cardinality categorical columns (if <10 class one-hot, otherwise target)\n",
    "for c in [\n",
    "    #\"homeTeamCode\", 32 classes\n",
    "    #\"awayTeamCode\", 32 classes\n",
    "    \"team\", # 2 classes\n",
    "    \"location\", # 3 classes\n",
    "    \"shotType\", # 7 classes\n",
    "    #\"lastEventCategory\", 17 classes\n",
    "    #\"lastEventTeam\", 30 classes\n",
    "    ##\"goalieNameForShot\", 237 classes\n",
    "    ##\"shooterName\", 2017 classes\n",
    "    \"shooterLeftRight\", # 2 classes\n",
    "    \"playerPositionThatDidEvent\", # 5 classes\n",
    "    #\"teamCode\" 32 classes\n",
    "]:\n",
    "    col_enc_df = pd.get_dummies(_df[c], sparse=True).reindex(columns=_df[c].unique(), fill_value=False).astype(int).add_prefix(f\"{c}_\")\n",
    "    _df = pd.concat([_df, col_enc_df], axis = 1)\n",
    "    # Drop the unencoded column\n",
    "    _df.drop([c], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"num rows in encoded dataframe: {len(_df)}\")\n",
    "print(f\"num cols in encoded dataframe: {len(_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02e88e31-709c-46d8-bb11-6b947f35cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in encoded dataframe: 387932\n",
      "num cols in encoded dataframe: 123\n"
     ]
    }
   ],
   "source": [
    "# Calculate the smoothed target encoding for high cardinality categorical columns\n",
    "for c in [\n",
    "    \"homeTeamCode\", # 32 classes\n",
    "    \"awayTeamCode\", # 32 classes\n",
    "    # \"team\", 2 classes\n",
    "    # \"location\", 3 classes\n",
    "    # \"shotType\", 7 classes\n",
    "    \"lastEventCategory\", # 17 classes\n",
    "    \"lastEventTeam\", # 30 classes\n",
    "    \"goalieNameForShot\", # 237 classes\n",
    "    \"shooterName\", # 2017 classes\n",
    "    # \"shooterLeftRight\", 2 classes\n",
    "    # \"playerPositionThatDidEvent\", 5 classes\n",
    "    \"teamCode\" # 32 classes\n",
    "]:\n",
    "    m = len(_df[c])\n",
    "    _df[f'{c}_target'] = calc_smooth_mean(_df, by=c, on='goal', m=m)\n",
    "    \n",
    "print(f\"num rows in encoded dataframe: {len(_df)}\")\n",
    "print(f\"num cols in encoded dataframe: {len(_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4e65aa-c4ea-4717-9875-508ee24dfd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save target encodings\n",
    "_df[[\n",
    "    \"homeTeamCode\",\n",
    "    \"awayTeamCode\",\n",
    "    # \"team\",\n",
    "    # \"location\",\n",
    "    # \"shotType\",\n",
    "    \"lastEventCategory\",\n",
    "    \"lastEventTeam\",\n",
    "    \"goalieNameForShot\",\n",
    "    \"shooterName\",\n",
    "    # \"shooterLeftRight\",\n",
    "    # \"playerPositionThatDidEvent\",\n",
    "    \"teamCode\",\n",
    "    \"homeTeamCode_target\",\n",
    "    \"awayTeamCode_target\",\n",
    "    # \"team_target\",\n",
    "    # \"location_target\",\n",
    "    # \"shotType_target\",\n",
    "    \"lastEventCategory_target\",\n",
    "    \"lastEventTeam_target\",\n",
    "    \"goalieNameForShot_target\",\n",
    "    \"shooterName_target\",\n",
    "    # \"shooterLeftRight_target\",\n",
    "    # \"playerPositionThatDidEvent_target\",\n",
    "    \"teamCode_target\"\n",
    "]].to_csv(\"target_encoding_lookups.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f356aef1-61a8-466b-8986-baa9be8f53a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num cols in encoded dataframe: 115\n"
     ]
    }
   ],
   "source": [
    "# Drop encoded columns\n",
    "_df_reduced = _df.drop([ \"homeTeamCode\",\n",
    "    \"awayTeamCode\",\n",
    "    \"team\",\n",
    "    \"location\",\n",
    "    \"shotType\",\n",
    "    \"lastEventCategory\",\n",
    "    \"lastEventTeam\",\n",
    "    \"goalieNameForShot\",\n",
    "    \"shooterName\",\n",
    "    \"shooterLeftRight\",\n",
    "    \"playerPositionThatDidEvent\",\n",
    "    \"teamCode\",\n",
    "    \"game_id\"], axis=1, inplace=False, errors='ignore')\n",
    "print(f\"num cols in encoded dataframe: {len(_df_reduced.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f769cdb8-05a8-45f7-9549-c4a2943bf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoded data\n",
    "_df_reduced.to_csv(\"encoded_shot_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d80b05-41ba-481c-8115-9ccbf2540cd4",
   "metadata": {},
   "source": [
    "## Create Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982375b6-6a4b-4677-a3ab-f44c1ae9a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num rows in encoded dataframe: 387932\n",
      "num cols in encoded dataframe: 115\n"
     ]
    }
   ],
   "source": [
    "# Import encoded data\n",
    "_df_reduced = pd.read_csv(\"encoded_shot_data.csv\")\n",
    "print(f\"num rows in encoded dataframe: {len(_df_reduced)}\")\n",
    "print(f\"num cols in encoded dataframe: {len(_df_reduced.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8028d47f-218b-4ec8-ac67-301ceb5e0339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306283\n",
      "81649\n"
     ]
    }
   ],
   "source": [
    "# Create training and test sets\n",
    "train_df = _df_reduced[_df_reduced[\"season\"] != 2023]\n",
    "test_df = _df_reduced[_df_reduced[\"season\"] == 2023]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "\n",
    "X_train = train_df.loc[:, ~train_df.columns.isin([\"goal\", \"xGoal\"])]\n",
    "X_test = test_df.loc[:, ~test_df.columns.isin([\"goal\", \"xGoal\"])]\n",
    "y_train = train_df[\"goal\"]\n",
    "y_test = test_df[\"goal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b449e176-2cca-45d5-add3-9c85ccfd42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit columns to those used in the graph neural network\n",
    "limit_X_train = X_train[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                         \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "limit_X_test = X_test[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\",\n",
    "                      \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df8349-2d69-4d64-8c4f-76692f4dd02f",
   "metadata": {},
   "source": [
    "## Fit Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "781b42be-a1e3-4560-aa5d-d3b01a27f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            8     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.62777D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    8     48     57      1     0     0   7.173D-05   2.972D-01\n",
      "  F =  0.29722251828284807     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "lr = LogisticRegression(random_state = seed, max_iter = 500, verbose=1).fit(limit_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff321f7d-5d68-49f8-9944-e5cc6851004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression MSE:  0.08655198899196545\n",
      "Logistic Regression Accuracy:  0.8992639223995395\n"
     ]
    }
   ],
   "source": [
    "# Determine model performance\n",
    "preds = lr.predict_proba(limit_X_test)[:,1]\n",
    "print(\"Logistic Regression MSE: \", np.square(np.subtract(y_test,preds)).mean())\n",
    "print(\"Logistic Regression Accuracy: \", sum(lr.predict(limit_X_test) == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453fff6-d8f2-4132-bc81-84148685b909",
   "metadata": {},
   "source": [
    "## Fit xGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6355bddf-757c-4987-86bb-bfd1dd3ef312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into folds by year for 4-fold cross-validation\n",
    "#train1415_df = train_df[train_df[\"season\"] == 2014]\n",
    "#train1516_df = train_df[train_df[\"season\"] == 2015]\n",
    "#train1617_df = train_df[train_df[\"season\"] == 2016]\n",
    "#train1718_df = train_df[train_df[\"season\"] == 2017]\n",
    "#train1819_df = train_df[train_df[\"season\"] == 2018]\n",
    "train1920_df = train_df[train_df[\"season\"] == 2019]\n",
    "train2021_df = train_df[train_df[\"season\"] == 2020]\n",
    "train2122_df = train_df[train_df[\"season\"] == 2021]\n",
    "train2223_df = train_df[train_df[\"season\"] == 2022]\n",
    "\n",
    "#limit_X_train1415 = train1415_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "#                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "#limit_X_train1516 = train1516_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "#                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "#limit_X_train1617 = train1617_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "#                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "#limit_X_train1718 = train1718_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "#                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "#limit_X_train1819 = train1819_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "#                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "limit_X_train1920 = train1920_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "limit_X_train2021 = train2021_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "limit_X_train2122 = train2122_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "limit_X_train2223 = train2223_df[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                                    \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "\n",
    "#y_train1415 = train1415_df[\"goal\"]\n",
    "#y_train1516 = train1516_df[\"goal\"]\n",
    "#y_train1617 = train1617_df[\"goal\"]\n",
    "#y_train1718 = train1718_df[\"goal\"]\n",
    "#y_train1819 = train1819_df[\"goal\"]\n",
    "y_train1920 = train1920_df[\"goal\"]\n",
    "y_train2021 = train2021_df[\"goal\"]\n",
    "y_train2122 = train2122_df[\"goal\"]\n",
    "y_train2223 = train2223_df[\"goal\"]\n",
    "\n",
    "X_trains = [limit_X_train1920, limit_X_train2021, limit_X_train2122, limit_X_train2223]#[limit_X_train1415, limit_X_train1516, limit_X_train1617, limit_X_train1718, limit_X_train1819, limit_X_train1920, limit_X_train2021, limit_X_train2122, limit_X_train2223]\n",
    "y_trains = [y_train1920, y_train2021, y_train2122, y_train2223]#[y_train1415, y_train1516, y_train1617, y_train1718, y_train1819, y_train1920, y_train2021, y_train2122, y_train2223]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e91993c-e071-4702-8745-ec648b117ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0  Max Depth:  1  Learning Rate:  0.05  MSE:  0.08240476876422415 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.1  MSE:  0.08171809288414028 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.15  MSE:  0.08147989649309799 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.2  MSE:  0.08138114568067864 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.25  MSE:  0.08134818559938653 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.3  MSE:  0.0813319311786151 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.35  MSE:  0.0813146565918271 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  1  Learning Rate:  0.4  MSE:  0.08132091674878733 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.05  MSE:  0.08160830119642921 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.1  MSE:  0.08127220500129456 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.15  MSE:  0.08122691597758196 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.2  MSE:  0.08122127759216667 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.25  MSE:  0.08123954022399847 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.3  MSE:  0.08122692356316588 Accuracy:  0.9049222893115654\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.35  MSE:  0.08124377860288028 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  2  Learning Rate:  0.4  MSE:  0.0812422786008886 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.05  MSE:  0.08138207004347425 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.1  MSE:  0.0812434474972557 Accuracy:  0.9049222893115654\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.15  MSE:  0.08122421575101041 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.2  MSE:  0.08123585659467872 Accuracy:  0.9049222893115654\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.25  MSE:  0.08125279287356191 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.3  MSE:  0.08133733906529156 Accuracy:  0.9049590319538512\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.35  MSE:  0.08133602374531691 Accuracy:  0.9048977942167081\n",
      "Fold: 0  Max Depth:  3  Learning Rate:  0.4  MSE:  0.08133811616337047 Accuracy:  0.9048610515744222\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.05  MSE:  0.08129920954586459 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.1  MSE:  0.08123938056886178 Accuracy:  0.9049100417641367\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.15  MSE:  0.08125622658066238 Accuracy:  0.9049467844064226\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.2  MSE:  0.08130379763124429 Accuracy:  0.9049467844064226\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.25  MSE:  0.08134614565345352 Accuracy:  0.9048732991218509\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.3  MSE:  0.0814222345487035 Accuracy:  0.9048120613847077\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.35  MSE:  0.08152436076941427 Accuracy:  0.9047263285527073\n",
      "Fold: 0  Max Depth:  4  Learning Rate:  0.4  MSE:  0.08167099867548377 Accuracy:  0.9047875662898505\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.05  MSE:  0.08243622588006162 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.1  MSE:  0.08169383550854611 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.15  MSE:  0.08147135083191154 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.2  MSE:  0.08135543798320453 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.25  MSE:  0.08133421846225841 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.3  MSE:  0.0813210080211276 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.35  MSE:  0.08131680626674849 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  1  Learning Rate:  0.4  MSE:  0.08133184853559174 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.05  MSE:  0.08157878028563288 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.1  MSE:  0.08128299113820411 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.15  MSE:  0.08121419362289269 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.2  MSE:  0.08120209579054183 Accuracy:  0.9048977942167081\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.25  MSE:  0.08119247378630594 Accuracy:  0.9048855466692795\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.3  MSE:  0.081201923185562 Accuracy:  0.9048977942167081\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.35  MSE:  0.08117629563488904 Accuracy:  0.9048610515744222\n",
      "Fold: 1  Max Depth:  2  Learning Rate:  0.4  MSE:  0.08124469508422917 Accuracy:  0.9049222893115654\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.05  MSE:  0.08134654157869808 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.1  MSE:  0.08120290296549186 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.15  MSE:  0.08118164976334617 Accuracy:  0.9048977942167081\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.2  MSE:  0.08119992691149153 Accuracy:  0.9049222893115654\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.25  MSE:  0.08118697688606442 Accuracy:  0.9049590319538512\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.3  MSE:  0.08120730835960703 Accuracy:  0.9049712795012799\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.35  MSE:  0.08123775192963224 Accuracy:  0.9049957745961371\n",
      "Fold: 1  Max Depth:  3  Learning Rate:  0.4  MSE:  0.08127376636056784 Accuracy:  0.9048977942167081\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.05  MSE:  0.08126712689207974 Accuracy:  0.9049100417641367\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.1  MSE:  0.0812189981523682 Accuracy:  0.9048855466692795\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.15  MSE:  0.08120400055915357 Accuracy:  0.9049222893115654\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.2  MSE:  0.08123486620741974 Accuracy:  0.9049467844064226\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.25  MSE:  0.08136558872530601 Accuracy:  0.9049345368589939\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.3  MSE:  0.08133911891906777 Accuracy:  0.9049712795012799\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.35  MSE:  0.08137335492228094 Accuracy:  0.9048732991218509\n",
      "Fold: 1  Max Depth:  4  Learning Rate:  0.4  MSE:  0.08160209483333324 Accuracy:  0.9047630711949932\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.05  MSE:  0.08244257873288083 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.1  MSE:  0.08170390465327826 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.15  MSE:  0.08149647474570015 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.2  MSE:  0.08140973956364628 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.25  MSE:  0.08137364822289235 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.3  MSE:  0.08136172167808685 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.35  MSE:  0.08136708379438003 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  1  Learning Rate:  0.4  MSE:  0.08135176785693327 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.05  MSE:  0.08161239579588463 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.1  MSE:  0.08128622274851734 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.15  MSE:  0.08124897805131917 Accuracy:  0.9048977942167081\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.2  MSE:  0.08124617037069005 Accuracy:  0.9048977942167081\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.25  MSE:  0.08124557254159923 Accuracy:  0.9049222893115654\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.3  MSE:  0.08127982345028908 Accuracy:  0.9048855466692795\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.35  MSE:  0.08128450898524513 Accuracy:  0.9048732991218509\n",
      "Fold: 2  Max Depth:  2  Learning Rate:  0.4  MSE:  0.08129023255324037 Accuracy:  0.9048732991218509\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.05  MSE:  0.08136522402125373 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.1  MSE:  0.08124669622626184 Accuracy:  0.9049222893115654\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.15  MSE:  0.08123760461327821 Accuracy:  0.9048977942167081\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.2  MSE:  0.08125192003545571 Accuracy:  0.9048977942167081\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.25  MSE:  0.08127451820279795 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.3  MSE:  0.08133665402790441 Accuracy:  0.9048855466692795\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.35  MSE:  0.08137876845824081 Accuracy:  0.904799813837279\n",
      "Fold: 2  Max Depth:  3  Learning Rate:  0.4  MSE:  0.08140931478794218 Accuracy:  0.904738576100136\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.05  MSE:  0.08129409263097144 Accuracy:  0.9049100417641367\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.1  MSE:  0.0812513350198668 Accuracy:  0.9048610515744222\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.15  MSE:  0.08128928600240799 Accuracy:  0.9048243089321364\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.2  MSE:  0.08134432225393154 Accuracy:  0.9048243089321364\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.25  MSE:  0.08139691345109604 Accuracy:  0.9047630711949932\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.3  MSE:  0.08149343191345779 Accuracy:  0.904799813837279\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.35  MSE:  0.08162036774500375 Accuracy:  0.9047140810052787\n",
      "Fold: 2  Max Depth:  4  Learning Rate:  0.4  MSE:  0.08173592614231813 Accuracy:  0.9045181202464206\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.05  MSE:  0.08241382860923378 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.1  MSE:  0.08167789187364938 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.15  MSE:  0.08147518603430284 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.2  MSE:  0.08141595035169097 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.25  MSE:  0.08140373495656303 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.3  MSE:  0.08139343297205281 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.35  MSE:  0.0814026973534827 Accuracy:  0.9048977942167081\n",
      "Fold: 3  Max Depth:  1  Learning Rate:  0.4  MSE:  0.08140945645295453 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.05  MSE:  0.08156481784683808 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.1  MSE:  0.08135686406205236 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.15  MSE:  0.08136098839585763 Accuracy:  0.9048610515744222\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.2  MSE:  0.08134306254605089 Accuracy:  0.9048977942167081\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.25  MSE:  0.0813732245570481 Accuracy:  0.9049467844064226\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.3  MSE:  0.08137974967543433 Accuracy:  0.9049222893115654\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.35  MSE:  0.08142077536974154 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  2  Learning Rate:  0.4  MSE:  0.08140157931992283 Accuracy:  0.9049467844064226\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.05  MSE:  0.08137918116694684 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.1  MSE:  0.0813361063084522 Accuracy:  0.9048977942167081\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.15  MSE:  0.08133587015521947 Accuracy:  0.9049222893115654\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.2  MSE:  0.08136825289613918 Accuracy:  0.9049835270487085\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.25  MSE:  0.08142444444104188 Accuracy:  0.9048610515744222\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.3  MSE:  0.08144094587876195 Accuracy:  0.9048120613847077\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.35  MSE:  0.08152969174605919 Accuracy:  0.9046650908155642\n",
      "Fold: 3  Max Depth:  3  Learning Rate:  0.4  MSE:  0.08152626682420344 Accuracy:  0.9048610515744222\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.05  MSE:  0.0813393478927487 Accuracy:  0.9049100417641367\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.1  MSE:  0.08133655967934006 Accuracy:  0.9049222893115654\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.15  MSE:  0.08139388872358728 Accuracy:  0.9049345368589939\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.2  MSE:  0.0814915949898284 Accuracy:  0.9049222893115654\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.25  MSE:  0.08157159470706286 Accuracy:  0.9047753187424218\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.3  MSE:  0.08166260955864897 Accuracy:  0.9047140810052787\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.35  MSE:  0.08173976692933911 Accuracy:  0.9047140810052787\n",
      "Fold: 3  Max Depth:  4  Learning Rate:  0.4  MSE:  0.08187957748874125 Accuracy:  0.90470183345785\n"
     ]
    }
   ],
   "source": [
    "# Intialize MSE and accuracy storage\n",
    "mses = [[0 for _ in range(4)] for _ in range(32)]\n",
    "accs = [[0 for _ in range(4)] for _ in range(32)]\n",
    "\n",
    "# Loop through folds\n",
    "for fold in range(0, len(X_trains)):\n",
    "    # Setup data for that fold\n",
    "    fold_inds = [i for i in range(len(X_trains)) if i != fold]\n",
    "    fold_dfs = [X_trains[i] for i in fold_inds]\n",
    "    fold_ys = [y_trains[i] for i in fold_inds]\n",
    "    all_fold_dfs = pd.concat(fold_dfs)\n",
    "    all_fold_ys = pd.concat(fold_ys)\n",
    "    count = 0\n",
    "    # Loop though possible max depths\n",
    "    for m_depth in [1,2,3,4]:\n",
    "        # Loop through possible learning rates\n",
    "        for lrate in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]:\n",
    "            # Fit classifier\n",
    "            xgb_clf = xgb.XGBClassifier(learning_rate = lrate, max_depth = m_depth)\n",
    "            xgb_clf.fit(all_fold_dfs, all_fold_ys)\n",
    "            # Make predictions\n",
    "            # Store performance metrics\n",
    "            xgb_preds = xgb_clf.predict_proba(limit_X_test)[:,1]\n",
    "            mses[count][fold] = np.square(np.subtract(y_test,xgb_preds)).mean()\n",
    "            accs[count][fold] = sum(xgb_clf.predict(limit_X_test) == y_test)/len(y_test)\n",
    "            print(\"Fold:\", fold,\" Max Depth: \", m_depth, \" Learning Rate: \", lrate, \" MSE: \", mses[count][fold], \"Accuracy: \", accs[count][fold])\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31434f5a-5b95-4232-8e07-c855c7b6a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# Choose hyperparameters that minimize average MSE across folds\n",
    "avg_mses = [0] * len(mses)\n",
    "for i in range(0,len(mses)):\n",
    "    avg_mses[i] = sum(mses[i])/len(mses[i])\n",
    "\n",
    "# Optimal is max depth = 3, lr = 0.15\n",
    "print(min(range(len(avg_mses)), key=avg_mses.__getitem__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e7c1a45-fae3-418c-a443-e1a297863a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xGBoost MSE:  0.08122035048892716\n",
      "xGBoost Accuracy:  0.9049222893115654\n"
     ]
    }
   ],
   "source": [
    "# Fit optimal xGBoost model and check performance on testing data\n",
    "opt_xgb_clf = xgb.XGBClassifier(learning_rate = 0.15, max_depth = 3)\n",
    "opt_xgb_clf.fit(limit_X_train, y_train)\n",
    "\n",
    "preds = opt_xgb_clf.predict_proba(limit_X_test)[:,1]\n",
    "print(\"xGBoost MSE: \", np.square(np.subtract(y_test,preds)).mean())\n",
    "print(\"xGBoost Accuracy: \", sum(opt_xgb_clf.predict(limit_X_test) == y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ba940-b78c-42da-934a-a53202d22d6b",
   "metadata": {},
   "source": [
    "## Create two node graph data for graph neural network (GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfee2315-4a5c-4ec9-bf3b-6628aef7c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to generate ShotDataset\n",
    "class ShotDataset(Dataset):\n",
    "    def __init__(self, root, filename, transform=None, pre_transform=None, pre_filter=None):\n",
    "        \"\"\"\n",
    "        root: where the dataset should be stored, folder is split into raw_dir (downloaded dataset)\n",
    "        and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        super(ShotDataset, self).__init__(root, transform=None, pre_transform=None, pre_filter=None)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        If this file exists in raw_dir, the download is not triggered.\n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Not implemented\n",
    "        \"\"\"\n",
    "        return 'xxxxx.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, shot in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(shot)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(shot)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(shot)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(shot[\"goal\"])\n",
    "\n",
    "        \n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_feats,y=label)\n",
    "            torch.save(data, os.path.join(self.processed_dir,f'shot_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, shot):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of the shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Initialize features storage\n",
    "        all_node_feats = []\n",
    "        shooter_feats = []\n",
    "        goalie_feats = []\n",
    "\n",
    "        # Get shooter features\n",
    "        shooter_feats.append(shot[\"shooterName_target\"])\n",
    "        shooter_feats.append(shot[\"shotType_WRIST\"])\n",
    "        shooter_feats.append(shot[\"shotAngleReboundRoyalRoad\"])\n",
    "        all_node_feats.append(shooter_feats)\n",
    "\n",
    "        # Get goalie features\n",
    "        goalie_feats.append(shot[\"goalieNameForShot_target\"])\n",
    "        goalie_feats.append(shot[\"defendingTeamAverageTimeOnIce\"])\n",
    "        goalie_feats.append(shot[\"defendingTeamMaxTimeOnIceOfDefencemen\"])\n",
    "        all_node_feats.append(goalie_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, shot):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of ths shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Initialize feature storage\n",
    "        all_edge_feats = []\n",
    "        edge_feats = []\n",
    "        # Get edge features\n",
    "        edge_feats.append(shot[\"shotDistance\"])\n",
    "        all_edge_feats += [edge_feats, edge_feats]\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, shot):\n",
    "        # Create adjacency matrix\n",
    "        edge_indices = []\n",
    "        edge_indices += [[0,1],[1,0]]\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        # Get data label (goal or no goal)\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'shot_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "955ebafb-ab81-490e-8a95-158137486385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small subset\n",
    "# Save the data to the folder to create graphs\n",
    "train_dataset = _df_reduced[_df_reduced[\"season\"] != 2023]\n",
    "test_dataset = _df_reduced[_df_reduced[\"season\"] == 2023]\n",
    "train_dataset.to_csv(\"COSC5P30/raw/train_dataset.csv\", index = False)\n",
    "test_dataset.to_csv(\"COSC5P30/raw/test_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6124d38a-05d6-41ec-a06c-4859851a51f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████| 306283/306283 [01:28<00:00, 3441.56it/s]\n",
      "Done!\n",
      "Processing...\n",
      "100%|███████████████████████████████████| 81649/81649 [00:23<00:00, 3432.04it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create the graphs\n",
    "train_dat = ShotDataset(filename = \"train_dataset.csv\", root=\"COSC5P30/\")\n",
    "test_dat = ShotDataset(filename = \"test_dataset.csv\", root=\"COSC5P30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5efce22-73bf-4576-b347-15f847bddcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[ 0.0975,  1.0000,  0.0000],\n",
      "        [ 0.0973, 30.0000, 33.0000]]))\n",
      "('edge_index', tensor([[0, 1],\n",
      "        [1, 0]]))\n",
      "('edge_attr', tensor([[39.6989],\n",
      "        [39.6989]]))\n",
      "('y', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "# Check what the data looks like\n",
    "for p in train_dat[0]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ddfacad-f242-4dc9-b35f-44aa2ce95f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ShotDataset(306283):\n",
      "====================\n",
      "Number of graphs: 306283\n",
      "Number of features: 3\n",
      "\n",
      "Data(x=[2, 3], edge_index=[2, 2], edge_attr=[2, 1], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 2\n",
      "Number of undirected edges: 1\n",
      "Average node degree: 1.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(f'Dataset: {train_dat}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(train_dat)}')\n",
    "print(f'Number of features: {train_dat.num_features}')\n",
    "#print(f'Number of classes: {train_dat.num_classes}')\n",
    "\n",
    "data = train_dat[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of undirected edges: {data.num_edges // 2}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "812fa48b-bc3c-46cf-b4f4-078e4ebb009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 306283\n",
      "Number of test graphs: 81649\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training graphs: {len(train_dat)}')\n",
    "print(f'Number of test graphs: {len(test_dat)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93d3019a-6585-4cc9-a90a-af204b9d19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dat, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dat, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35eddf-e9c7-4b3c-923e-9bd5506f0357",
   "metadata": {},
   "source": [
    "## Fit two node GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "791f1425-2367-43b4-aee2-efb855189907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(train_dat.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels,1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return torch.sigmoid(self.lin(x))\n",
    "\n",
    "# choose either 64 or 128 hidden channels\n",
    "model = GCN(hidden_channels=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54417771-8d3f-47cc-8f87-8ad53c7fb692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000000, Train Acc: 0.891538, Test Acc: 0.888633, Test MSE: 0.110602\n",
      "Epoch: 000001, Train Acc: 0.899622, Test Acc: 0.897427, Test MSE: 0.102481\n",
      "Epoch: 000002, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100703\n",
      "Epoch: 000003, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100648\n",
      "Epoch: 000004, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100727\n",
      "Epoch: 000005, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100726\n",
      "Epoch: 000006, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100731\n",
      "Epoch: 000007, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100731\n",
      "Epoch: 000008, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100733\n",
      "Epoch: 000009, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100733\n",
      "Epoch: 000010, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100733\n",
      "Epoch: 000011, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000012, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000013, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000014, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000015, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000016, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100735\n",
      "Epoch: 000017, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100736\n",
      "Epoch: 000018, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m sq_error \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset), correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset), pred, data\u001b[38;5;241m.\u001b[39my, goals\u001b[38;5;66;03m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     train()\n\u001b[1;32m     35\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[1;32m     36\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m          out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m          \u001b[38;5;66;03m#print(\"out: \", out)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m          \u001b[38;5;66;03m#print(\"y: \", data.y)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[25], line 91\u001b[0m, in \u001b[0;36mShotDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 91\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1066\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m-> 1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1071\u001b[0m         overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:206\u001b[0m, in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Read the first few bytes and match against the ZIP file signature\u001b[39;00m\n\u001b[1;32m    205\u001b[0m local_header_magic_number \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 206\u001b[0m read_bytes \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(local_header_magic_number))\n\u001b[1;32m    207\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m read_bytes \u001b[38;5;241m==\u001b[39m local_header_magic_number\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# create training function\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    # Iterate through data loader\n",
    "    for data in train_loader:  \n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(out, data.y.unsqueeze(1).float())\n",
    "        # Derive gradient\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# create testing function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    sq_error = 0\n",
    "    correct = 0\n",
    "    goals = 0\n",
    "\n",
    "    # Iterate through data loader\n",
    "    for data in loader:\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        # Make prediction\n",
    "        pred = out\n",
    "        # Compute metrics\n",
    "        sq_error += (((pred - data.y.unsqueeze(1)))**2).sum()\n",
    "        correct += (torch.round(pred) == data.y.unsqueeze(1)).sum()\n",
    "        goals += sum(data.y.unsqueeze(1))\n",
    "    return sq_error / len(loader.dataset), correct / len(loader.dataset), pred, data.y, goals\n",
    "\n",
    "\n",
    "for epoch in range(0, 30):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:06d}, Train Acc: {train_acc[1].item():.6f}, Test Acc: {test_acc[1].item():.6f}, Test MSE: {test_acc[0].item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a99b87-1953-4b4e-b4d0-3398ba0de9a8",
   "metadata": {},
   "source": [
    "## Create graph data for graph neural network (GNN) with all players on the ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d0b2663-82ab-4413-921f-d83d17f9923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class to generate 12 node graphs\n",
    "class MultiPlayerShotDataset(Dataset):\n",
    "    def __init__(self, root, filename, transform=None, pre_transform=None, pre_filter=None):\n",
    "        \"\"\"\n",
    "        root: where the dataset should be stored, folder is split into raw_dir (downloaded dataset)\n",
    "        and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        super(MultiPlayerShotDataset, self).__init__(root, transform=None, pre_transform=None, pre_filter=None)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        If this file exists in raw_dir, the download is not triggered.\n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Not implemented\n",
    "        \"\"\"\n",
    "        return 'xxxxx.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, shot in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            # get number of players on the ice for shot\n",
    "            n_forwards_shooting_team = int(shot[\"shootingTeamForwardsOnIce\"])\n",
    "            n_forwards_defending_team = int(shot[\"defendingTeamForwardsOnIce\"])\n",
    "            n_defencemen_shooting_team = int(shot[\"shootingTeamDefencemenOnIce\"])\n",
    "            n_defencemen_defending_team = int(shot[\"defendingTeamDefencemenOnIce\"])\n",
    "            n_players_on_ice = n_forwards_shooting_team + n_forwards_defending_team + n_defencemen_shooting_team + n_defencemen_defending_team\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(shot, n_forwards_shooting_team, n_forwards_defending_team, n_defencemen_shooting_team, n_defencemen_defending_team)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(shot, n_players_on_ice + 1)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(shot)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(shot[\"goal\"])\n",
    "\n",
    "        \n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, edge_index=edge_index.t().contiguous(), edge_attr=edge_feats,y=label)\n",
    "            torch.save(data, os.path.join(self.processed_dir,f'shot_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, shot, n_forwards_shooting_team, n_forwards_defending_team, n_defencemen_shooting_team, n_defencemen_defending_team):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of the shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # set goalie and shooter features\n",
    "        all_node_feats = []\n",
    "        shooter_feats = [shot[\"shooterName_target\"], shot[\"shotType_WRIST\"], shot[\"shotAngleReboundRoyalRoad\"]]\n",
    "        goalie_feats = [shot[\"goalieNameForShot_target\"], shot[\"defendingTeamAverageTimeOnIce\"], shot[\"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "        all_node_feats.append(shooter_feats)\n",
    "        all_node_feats.append(goalie_feats)\n",
    "\n",
    "        # Create the four different categories of adjacent player, checking if shot was from a defencemen or not\n",
    "        if shot[\"playerPositionThatDidEvent_D\"] == 1:\n",
    "            for i in range(n_defencemen_shooting_team - 1):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfDefencemen\"], shot[\"shootingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for j in range(n_forwards_shooting_team):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfForwards\"], shot[\"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for k in range(n_defencemen_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfDefencemen\"], shot[\"defendingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for l in range(n_forwards_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfForwards\"], shot[\"defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "        elif shot[\"playerPositionThatDidEvent_D\"] == 0:\n",
    "            for i in range(n_defencemen_shooting_team):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfDefencemen\"], shot[\"shootingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for j in range(n_forwards_shooting_team - 1):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfForwards\"], shot[\"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for k in range(n_defencemen_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfDefencemen\"], shot[\"defendingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for l in range(n_forwards_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfForwards\"], shot[\"defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "\n",
    "        # Add padding if not maximum players\n",
    "        n_player_nodes = n_defencemen_shooting_team + n_forwards_shooting_team + n_defencemen_defending_team + n_forwards_defending_team\n",
    "        if n_player_nodes < 11:\n",
    "            for i in range(11 - n_player_nodes):\n",
    "                adj_player_feats = [0, 0, 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, shot, n_players_on_ice):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of ths shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Get edge features\n",
    "        all_edge_feats = []\n",
    "        edge_feats = [shot[\"shotDistance\"]]\n",
    "        all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        # Get random edge features for adjacent players\n",
    "        for i in range(0, n_players_on_ice):\n",
    "            for j in range(i+1, n_players_on_ice):\n",
    "                if i == 0 and j == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    random_distance = random.uniform(0, 75)\n",
    "                    edge_feats = [random_distance]\n",
    "                    all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        # Add padding if not maximum players\n",
    "        if n_players_on_ice < 12:\n",
    "            for i in range(0, n_players_on_ice):\n",
    "                for j in range(n_players_on_ice, 12):\n",
    "                    edge_feats = [0]\n",
    "                    all_edge_feats += [edge_feats, edge_feats]\n",
    "                \n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, shot):\n",
    "        edge_indices = []\n",
    "\n",
    "        # There are 12 fully connected nodes in the adjacency matrix\n",
    "        for i in range(0, 12):\n",
    "            for j in range(i+1, 12):\n",
    "                edge_indices.append([i, j])\n",
    "                edge_indices.append([j, i])\n",
    "        edge_indices = torch.tensor(edge_indices, dtype=torch.long)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        # Get label (goal or no goal)\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'shot_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "833cf733-b27d-4aad-81cc-edd2acc8de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small subset of data to make graphs\n",
    "train_dataset_adj = _df_reduced[_df_reduced[\"season\"] != 2023]\n",
    "test_dataset_adj = _df_reduced[_df_reduced[\"season\"] == 2023]\n",
    "train_dataset_adj.to_csv(\"COSC5P30/raw/train_dataset_adj.csv\", index = False)\n",
    "test_dataset_adj.to_csv(\"COSC5P30/raw/test_dataset_adj.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6376ea4-b9da-4dc7-8058-10a505f194a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████| 306283/306283 [02:09<00:00, 2363.23it/s]\n",
      "Done!\n",
      "Processing...\n",
      "100%|███████████████████████████████████| 81649/81649 [00:33<00:00, 2414.25it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create the graphs\n",
    "train_dat_adj = MultiPlayerShotDataset(filename = \"train_dataset_adj.csv\", root=\"COSC5P30/\")\n",
    "test_dat_adj = MultiPlayerShotDataset(filename = \"test_dataset_adj.csv\", root=\"COSC5P30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e98383f-8e35-46af-a29c-0888599f1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[ 0.0975,  1.0000,  0.0000],\n",
      "        [ 0.0973, 30.0000, 33.0000],\n",
      "        [20.5000, 20.5000,  0.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [32.0000, 26.0000,  0.0000],\n",
      "        [32.0000, 26.0000,  0.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]]))\n",
      "('edge_index', tensor([[ 0,  1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,\n",
      "          0, 10,  0, 11,  1,  2,  1,  3,  1,  4,  1,  5,  1,  6,  1,  7,  1,  8,\n",
      "          1,  9,  1, 10,  1, 11,  2,  3,  2,  4,  2,  5,  2,  6,  2,  7,  2,  8,\n",
      "          2,  9,  2, 10,  2, 11,  3,  4,  3,  5,  3,  6,  3,  7,  3,  8,  3,  9,\n",
      "          3, 10,  3, 11,  4,  5,  4,  6,  4,  7,  4,  8,  4,  9,  4, 10,  4, 11,\n",
      "          5,  6,  5,  7,  5,  8,  5,  9,  5, 10,  5, 11,  6,  7,  6,  8,  6,  9,\n",
      "          6, 10,  6, 11,  7,  8,  7,  9,  7, 10,  7, 11,  8,  9,  8, 10,  8, 11,\n",
      "          9, 10,  9, 11, 10, 11],\n",
      "        [ 1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,  0,\n",
      "         10,  0, 11,  0,  2,  1,  3,  1,  4,  1,  5,  1,  6,  1,  7,  1,  8,  1,\n",
      "          9,  1, 10,  1, 11,  1,  3,  2,  4,  2,  5,  2,  6,  2,  7,  2,  8,  2,\n",
      "          9,  2, 10,  2, 11,  2,  4,  3,  5,  3,  6,  3,  7,  3,  8,  3,  9,  3,\n",
      "         10,  3, 11,  3,  5,  4,  6,  4,  7,  4,  8,  4,  9,  4, 10,  4, 11,  4,\n",
      "          6,  5,  7,  5,  8,  5,  9,  5, 10,  5, 11,  5,  7,  6,  8,  6,  9,  6,\n",
      "         10,  6, 11,  6,  8,  7,  9,  7, 10,  7, 11,  7,  9,  8, 10,  8, 11,  8,\n",
      "         10,  9, 11,  9, 11, 10]]))\n",
      "('edge_attr', tensor([[39.6989],\n",
      "        [39.6989],\n",
      "        [10.1488],\n",
      "        [10.1488],\n",
      "        [ 7.1176],\n",
      "        [ 7.1176],\n",
      "        [55.3631],\n",
      "        [55.3631],\n",
      "        [34.4540],\n",
      "        [34.4540],\n",
      "        [34.2076],\n",
      "        [34.2076],\n",
      "        [ 8.8801],\n",
      "        [ 8.8801],\n",
      "        [58.0180],\n",
      "        [58.0180],\n",
      "        [48.2514],\n",
      "        [48.2514],\n",
      "        [32.3036],\n",
      "        [32.3036],\n",
      "        [68.4273],\n",
      "        [68.4273],\n",
      "        [60.4632],\n",
      "        [60.4632],\n",
      "        [46.5850],\n",
      "        [46.5850],\n",
      "        [52.4974],\n",
      "        [52.4974],\n",
      "        [27.1155],\n",
      "        [27.1155],\n",
      "        [65.1797],\n",
      "        [65.1797],\n",
      "        [37.9632],\n",
      "        [37.9632],\n",
      "        [60.2047],\n",
      "        [60.2047],\n",
      "        [ 3.1354],\n",
      "        [ 3.1354],\n",
      "        [14.7887],\n",
      "        [14.7887],\n",
      "        [ 2.4264],\n",
      "        [ 2.4264],\n",
      "        [41.0917],\n",
      "        [41.0917],\n",
      "        [52.1613],\n",
      "        [52.1613],\n",
      "        [25.9781],\n",
      "        [25.9781],\n",
      "        [ 5.4550],\n",
      "        [ 5.4550],\n",
      "        [ 6.8228],\n",
      "        [ 6.8228],\n",
      "        [37.8402],\n",
      "        [37.8402],\n",
      "        [49.4130],\n",
      "        [49.4130],\n",
      "        [49.5890],\n",
      "        [49.5890],\n",
      "        [36.0985],\n",
      "        [36.0985],\n",
      "        [12.3959],\n",
      "        [12.3959],\n",
      "        [12.0979],\n",
      "        [12.0979],\n",
      "        [14.2907],\n",
      "        [14.2907],\n",
      "        [20.1830],\n",
      "        [20.1830],\n",
      "        [74.6546],\n",
      "        [74.6546],\n",
      "        [70.8359],\n",
      "        [70.8359],\n",
      "        [ 4.3890],\n",
      "        [ 4.3890],\n",
      "        [58.4642],\n",
      "        [58.4642],\n",
      "        [11.3340],\n",
      "        [11.3340],\n",
      "        [17.2265],\n",
      "        [17.2265],\n",
      "        [39.0079],\n",
      "        [39.0079],\n",
      "        [23.3038],\n",
      "        [23.3038],\n",
      "        [67.4181],\n",
      "        [67.4181],\n",
      "        [26.1568],\n",
      "        [26.1568],\n",
      "        [45.0848],\n",
      "        [45.0848],\n",
      "        [68.8783],\n",
      "        [68.8783],\n",
      "        [38.2690],\n",
      "        [38.2690],\n",
      "        [29.7053],\n",
      "        [29.7053],\n",
      "        [63.5022],\n",
      "        [63.5022],\n",
      "        [45.0805],\n",
      "        [45.0805],\n",
      "        [16.2915],\n",
      "        [16.2915],\n",
      "        [53.8970],\n",
      "        [53.8970],\n",
      "        [ 6.7267],\n",
      "        [ 6.7267],\n",
      "        [60.4955],\n",
      "        [60.4955],\n",
      "        [ 1.4494],\n",
      "        [ 1.4494],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]]))\n",
      "('y', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "# Check what the data looks like\n",
    "for p in train_dat_adj[0]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f17e563-8a1e-4fe3-a0c4-887e0d9ddfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MultiPlayerShotDataset(306283):\n",
      "====================\n",
      "Number of graphs: 306283\n",
      "Number of features: 3\n",
      "\n",
      "Data(x=[12, 3], edge_index=[2, 132], edge_attr=[132, 1], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 12\n",
      "Number of undirected edges: 66\n",
      "Average node degree: 11.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(f'Dataset: {train_dat_adj}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(train_dat_adj)}')\n",
    "print(f'Number of features: {train_dat_adj.num_features}')\n",
    "#print(f'Number of classes: {train_dat_adj.num_classes}')\n",
    "\n",
    "data = train_dat_adj[0]  # Get the first graph object\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of undirected edges: {data.num_edges // 2}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5357b23e-2f9a-4a76-9622-76166f2f6f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 306283\n",
      "Number of test graphs: 81649\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training graphs: {len(train_dat_adj)}')\n",
    "print(f'Number of test graphs: {len(test_dat_adj)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "485cbdec-78b5-46a3-b755-4a608ce0cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader_adj = DataLoader(train_dat_adj, batch_size=1024, shuffle=True)\n",
    "test_loader_adj = DataLoader(test_dat_adj, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3cf2b-5caf-4e3d-a594-33ef3a192cb4",
   "metadata": {},
   "source": [
    "## Fit GNN with adjacent players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8230fbd-cd5e-4847-b8f5-cfbea96db023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(train_dat_adj.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels,1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return torch.sigmoid(self.lin(x))\n",
    "\n",
    "# Choose between 64 and 128 hidden channels\n",
    "model = GCN(hidden_channels=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64ef3d98-9519-4063-a01b-534e72ed4e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000000, Train Acc: 0.799897, Test Acc: 0.788130, Test MSE: 0.194251\n",
      "Epoch: 000001, Train Acc: 0.838966, Test Acc: 0.826281, Test MSE: 0.161137\n",
      "Epoch: 000002, Train Acc: 0.861370, Test Acc: 0.848963, Test MSE: 0.139819\n",
      "Epoch: 000003, Train Acc: 0.871083, Test Acc: 0.859251, Test MSE: 0.136242\n",
      "Epoch: 000004, Train Acc: 0.882458, Test Acc: 0.871817, Test MSE: 0.125254\n",
      "Epoch: 000005, Train Acc: 0.888887, Test Acc: 0.880060, Test MSE: 0.117743\n",
      "Epoch: 000006, Train Acc: 0.890742, Test Acc: 0.882705, Test MSE: 0.114967\n",
      "Epoch: 000007, Train Acc: 0.892116, Test Acc: 0.884800, Test MSE: 0.114600\n",
      "Epoch: 000008, Train Acc: 0.892795, Test Acc: 0.885669, Test MSE: 0.113099\n",
      "Epoch: 000009, Train Acc: 0.895195, Test Acc: 0.889490, Test MSE: 0.109617\n",
      "Epoch: 000010, Train Acc: 0.896063, Test Acc: 0.890740, Test MSE: 0.108791\n",
      "Epoch: 000011, Train Acc: 0.896390, Test Acc: 0.891230, Test MSE: 0.108385\n",
      "Epoch: 000012, Train Acc: 0.898127, Test Acc: 0.893985, Test MSE: 0.105733\n",
      "Epoch: 000013, Train Acc: 0.897676, Test Acc: 0.893238, Test MSE: 0.106661\n",
      "Epoch: 000014, Train Acc: 0.898888, Test Acc: 0.894990, Test MSE: 0.104839\n",
      "Epoch: 000015, Train Acc: 0.899743, Test Acc: 0.896508, Test MSE: 0.103400\n",
      "Epoch: 000016, Train Acc: 0.899985, Test Acc: 0.896814, Test MSE: 0.103103\n",
      "Epoch: 000017, Train Acc: 0.900429, Test Acc: 0.897525, Test MSE: 0.102411\n",
      "Epoch: 000018, Train Acc: 0.900925, Test Acc: 0.898480, Test MSE: 0.101445\n",
      "Epoch: 000019, Train Acc: 0.901043, Test Acc: 0.898688, Test MSE: 0.101262\n",
      "Epoch: 000020, Train Acc: 0.901199, Test Acc: 0.898958, Test MSE: 0.100990\n",
      "Epoch: 000021, Train Acc: 0.901242, Test Acc: 0.899056, Test MSE: 0.100914\n",
      "Epoch: 000022, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100708\n",
      "Epoch: 000023, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100697\n",
      "Epoch: 000024, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100726\n",
      "Epoch: 000025, Train Acc: 0.901346, Test Acc: 0.899264, Test MSE: 0.100724\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 41\u001b[0m\n\u001b[1;32m     37\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m sq_error \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset), correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset), pred, data\u001b[38;5;241m.\u001b[39my, goals\u001b[38;5;66;03m# Derive ratio of correct predictions.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     train()\n\u001b[1;32m     42\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader_adj)\n\u001b[1;32m     43\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader_adj)\n",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader_adj:  \u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m          edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m      9\u001b[0m          edge_weight \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_attr  \u001b[38;5;66;03m# or data.edge_weight depending on your implementation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch_geometric/data/dataset.py:291\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"In case :obj:`idx` is of type integer, will return the data object\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mat index :obj:`idx` (and transforms it in case :obj:`transform` is\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03mpresent).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mbool, will return a subset of the dataset at the specified indices.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[0;32m--> 291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[1;32m    292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(data)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[25], line 142\u001b[0m, in \u001b[0;36mMultiPlayerShotDataset.get\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m--> 142\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1097\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1096\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1097\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1098\u001b[0m             opened_zipfile,\n\u001b[1;32m   1099\u001b[0m             map_location,\n\u001b[1;32m   1100\u001b[0m             pickle_module,\n\u001b[1;32m   1101\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1102\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1103\u001b[0m         )\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1105\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1525\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# Needed for tensors where storage device and rebuild tensor device are\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# not connected (wrapper subclasses and tensors rebuilt using numpy)\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1525\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_thread_local_state\u001b[38;5;241m.\u001b[39mmap_location\n\u001b[1;32m   1528\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/serialization.py:1508\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUnpicklerWrapper\u001b[39;00m(pickle_module\u001b[38;5;241m.\u001b[39mUnpickler):  \u001b[38;5;66;03m# type: ignore[name-defined]\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;66;03m# from https://stackoverflow.com/questions/13398462/unpickling-python-objects-with-a-changed-module-path/13405732\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m     \u001b[38;5;66;03m# Lets us override the imports that pickle uses when unpickling an object.\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m     \u001b[38;5;66;03m# This is useful for maintaining BC if we change a module path that tensor instantiation relies on.\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, mod_name, name):\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStorage\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[1;32m   1510\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create training function\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data loader\n",
    "    for data in train_loader_adj:  \n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # Loss computation\n",
    "        loss = criterion(out, data.y.unsqueeze(1).float())\n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "# Create testing function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    sq_error = 0\n",
    "    correct = 0\n",
    "    goals = 0\n",
    "\n",
    "    # Iterate over data loader\n",
    "    for data in loader:\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        # Make predictions\n",
    "        pred = out\n",
    "        # Compute metrics\n",
    "        sq_error += (((pred - data.y.unsqueeze(1)))**2).sum()\n",
    "        correct += (torch.round(pred) == data.y.unsqueeze(1)).sum()\n",
    "        goals += sum(data.y.unsqueeze(1))\n",
    "    return sq_error / len(loader.dataset), correct / len(loader.dataset), pred, data.y, goals\n",
    "\n",
    "\n",
    "for epoch in range(0, 30):\n",
    "    train()\n",
    "    train_acc = test(train_loader_adj)\n",
    "    test_acc = test(test_loader_adj)\n",
    "    print(f'Epoch: {epoch:06d}, Train Acc: {train_acc[1].item():.6f}, Test Acc: {test_acc[1].item():.6f}, Test MSE: {test_acc[0].item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c3d5ee-9010-4608-99aa-0f079d90e3ef",
   "metadata": {},
   "source": [
    "## Try models with Synthetic Minority Oversampling Technique (SMOTE) on the training set (using 2022 for training and 2023 for testing due to time limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41f094-1a3b-4f0a-bd8d-a2282d15170a",
   "metadata": {},
   "source": [
    "## SMOTE Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39797e9a-511e-4710-b5f6-335971717606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Size: 86240\n",
      "Original Test Size: 81649\n",
      "SMOTE Train Size: 155356\n",
      "SMOTE Test Size: 81649\n"
     ]
    }
   ],
   "source": [
    "# Create training and testing sets\n",
    "train_df = _df_reduced[_df_reduced[\"season\"] == 2022]\n",
    "test_df = _df_reduced[_df_reduced[\"season\"] == 2023]\n",
    "\n",
    "print(\"Original Train Size:\", len(train_df))\n",
    "print(\"Original Test Size:\", len(test_df))\n",
    "\n",
    "X_train = train_df.loc[:, ~train_df.columns.isin([\"goal\", \"xGoal\"])]\n",
    "X_test = test_df.loc[:, ~test_df.columns.isin([\"goal\", \"xGoal\"])]\n",
    "y_train = train_df[\"goal\"]\n",
    "y_test = test_df[\"goal\"]\n",
    "\n",
    "# Perform SMOTE\n",
    "smote = SMOTE(random_state = 12345)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"SMOTE Train Size:\", len(y_train_smote))\n",
    "print(\"SMOTE Test Size:\", len(test_df)) # doesn't get oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59c36ca9-88c6-490e-9248-a647283d2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit columns to those used in the graph neural network\n",
    "limit_X_train_smote = X_train_smote[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\", \n",
    "                         \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]\n",
    "limit_X_test_smote = X_test[[\"shooterName_target\", \"shotType_WRIST\", \"shotAngleReboundRoyalRoad\", \"goalieNameForShot_target\",\n",
    "                      \"defendingTeamAverageTimeOnIce\", \"defendingTeamMaxTimeOnIceOfDefencemen\", \"shotDistance\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40f30dd7-9d57-461d-a13b-1804cd802be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            8     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.15296D+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  5.89250D-01    |proj g|=  6.96236D-05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    8     50     60      1     0     0   6.962D-05   5.892D-01\n",
      "  F =  0.58924964495572663     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression\n",
    "lr = LogisticRegression(random_state = seed, max_iter = 500, verbose=1).fit(limit_X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fc50a99-2754-4eac-890d-099d85518edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression MSE:  0.20797548786088288\n",
      "Logistic Regression Accuracy:  0.6652500336807554\n",
      "Train Logistic Regression MSE:  0.20098287962261818\n",
      "Train Logistic Regression Accuracy:  0.6946883287417287\n"
     ]
    }
   ],
   "source": [
    "# Determine model performance\n",
    "preds = lr.predict_proba(limit_X_test_smote)[:,1]\n",
    "print(\"Logistic Regression MSE: \", np.square(np.subtract(y_test,preds)).mean())\n",
    "print(\"Logistic Regression Accuracy: \", sum(lr.predict(limit_X_test_smote) == y_test)/len(y_test))\n",
    "\n",
    "# Performance on training data\n",
    "train_preds = lr.predict_proba(limit_X_train_smote)[:,1]\n",
    "print(\"Train Logistic Regression MSE: \", np.square(np.subtract(y_train_smote,train_preds)).mean())\n",
    "print(\"Train Logistic Regression Accuracy: \", sum(lr.predict(limit_X_train_smote) == y_train_smote)/len(y_train_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febac0b-c993-41c1-9a48-16b9c188aaad",
   "metadata": {},
   "source": [
    "## SMOTE xGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c81ea14-21fe-46fe-a519-130bdec34cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split SMOTE data into four folds of size 155356/4 = 38839\n",
    "X_trains_smote = [limit_X_train_smote[:38839], limit_X_train_smote[38839:77678], limit_X_train_smote[77678:116517], limit_X_train_smote[116517:]]\n",
    "y_trains_smote = [y_train_smote[:38839], y_train_smote[38839:77678], y_train_smote[77678:116517], y_train_smote[116517:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34c9b991-ba0f-41e8-aa5f-ce32a5c4f5b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0  Max Depth:  6  Learning Rate:  0.25  MSE:  0.1697770084181263 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.3  MSE:  0.16565650789742675 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.35  MSE:  0.16491887279236966 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.4  MSE:  0.1641637750175 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.45  MSE:  0.16250512550374238 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.5  MSE:  0.16355182622428813 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.55  MSE:  0.16369131265960163 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  6  Learning Rate:  0.6  MSE:  0.16170862783994053 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.25  MSE:  0.1657832962669558 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.3  MSE:  0.16368638837375402 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.35  MSE:  0.16332171964475928 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.4  MSE:  0.1612418911165392 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.45  MSE:  0.16205873944707327 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.5  MSE:  0.1628504294379898 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.55  MSE:  0.16297988207754865 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  7  Learning Rate:  0.6  MSE:  0.1654423071261868 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.25  MSE:  0.1645097532707674 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.3  MSE:  0.16276482079546095 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.35  MSE:  0.16257079422843754 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.4  MSE:  0.1625883579895244 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.45  MSE:  0.16475331255979142 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.5  MSE:  0.1649247995376991 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.55  MSE:  0.16588434588435086 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  8  Learning Rate:  0.6  MSE:  0.16858372480582562 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.25  MSE:  0.16359159359751912 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.3  MSE:  0.16330771816312514 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.35  MSE:  0.16218461921957344 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.4  MSE:  0.16472252567883683 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.45  MSE:  0.1651570464673057 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.5  MSE:  0.16742075582613594 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.55  MSE:  0.16833036253123734 Accuracy:  0.90470183345785\n",
      "Fold: 0  Max Depth:  9  Learning Rate:  0.6  MSE:  0.17180456240244693 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.25  MSE:  0.1737866906486983 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.3  MSE:  0.16999223113570183 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.35  MSE:  0.17181358639164027 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.4  MSE:  0.1687207696541812 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.45  MSE:  0.1704003544280923 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.5  MSE:  0.17146630707683186 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.55  MSE:  0.17158008480275885 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  6  Learning Rate:  0.6  MSE:  0.171827946097023 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.25  MSE:  0.1711477575296106 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.3  MSE:  0.17122698129722827 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.35  MSE:  0.17188633447722237 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.4  MSE:  0.17079470629691468 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.45  MSE:  0.17065969824243477 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.5  MSE:  0.17083295986059394 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.55  MSE:  0.1731048507024462 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  7  Learning Rate:  0.6  MSE:  0.1740547716084952 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.25  MSE:  0.1695895129447967 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.3  MSE:  0.17072903491919403 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.35  MSE:  0.16967438143609803 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.4  MSE:  0.17017378545177717 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.45  MSE:  0.16988714490543338 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.5  MSE:  0.17282259816962353 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.55  MSE:  0.17355589112015396 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  8  Learning Rate:  0.6  MSE:  0.1747589095004359 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.25  MSE:  0.17029290331388486 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.3  MSE:  0.1700990698069161 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.35  MSE:  0.17224948235124102 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.4  MSE:  0.17326209572470205 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.45  MSE:  0.17467058485353784 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.5  MSE:  0.17558216923284223 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.55  MSE:  0.17813045221965604 Accuracy:  0.90470183345785\n",
      "Fold: 1  Max Depth:  9  Learning Rate:  0.6  MSE:  0.17911251088209498 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.25  MSE:  0.1259111481597542 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.3  MSE:  0.12452563006005223 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.35  MSE:  0.12693049416194102 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.4  MSE:  0.12632030385106 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.45  MSE:  0.1256757515612325 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.5  MSE:  0.12495294981208685 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.55  MSE:  0.12792943185772288 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  6  Learning Rate:  0.6  MSE:  0.12817890416801994 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.25  MSE:  0.12441812208519573 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.3  MSE:  0.12561836316033842 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.35  MSE:  0.12625212240808778 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.4  MSE:  0.1257847575036044 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.45  MSE:  0.12833693362917775 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.5  MSE:  0.12882656253559613 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.55  MSE:  0.12985756932095144 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  7  Learning Rate:  0.6  MSE:  0.12974546735151082 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.25  MSE:  0.1250735844414197 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.3  MSE:  0.12494293327829642 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.35  MSE:  0.12689834334234532 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.4  MSE:  0.1278179713979576 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.45  MSE:  0.12874550779480565 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.5  MSE:  0.13025768860708484 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.55  MSE:  0.13204441839742465 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  8  Learning Rate:  0.6  MSE:  0.13379043130382304 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.25  MSE:  0.1244254004849278 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.3  MSE:  0.12644620622108965 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.35  MSE:  0.12721914018592886 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.4  MSE:  0.12926922305164673 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.45  MSE:  0.1304993788123349 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.5  MSE:  0.13200483977944688 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.55  MSE:  0.1336940672209615 Accuracy:  0.90470183345785\n",
      "Fold: 2  Max Depth:  9  Learning Rate:  0.6  MSE:  0.13592858305161806 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.25  MSE:  0.11354307925897011 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.3  MSE:  0.11334874799613733 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.35  MSE:  0.11381384090630679 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.4  MSE:  0.11394693149765771 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.45  MSE:  0.11513012808216967 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.5  MSE:  0.11537039790559707 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.55  MSE:  0.11600943556822894 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  6  Learning Rate:  0.6  MSE:  0.11748743775864957 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.25  MSE:  0.11336877607267909 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.3  MSE:  0.11368560498344563 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.35  MSE:  0.11446921918501837 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.4  MSE:  0.11456063535600305 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.45  MSE:  0.11569419971640667 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.5  MSE:  0.11710853712502803 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.55  MSE:  0.11875125209613908 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  7  Learning Rate:  0.6  MSE:  0.11863410567694337 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.25  MSE:  0.11392008448838753 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.3  MSE:  0.11470510933561678 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.35  MSE:  0.11534895830723577 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.4  MSE:  0.1166248914996601 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.45  MSE:  0.11828858834505813 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.5  MSE:  0.11915352843060974 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.55  MSE:  0.12065343521599284 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  8  Learning Rate:  0.6  MSE:  0.12287551886003169 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.25  MSE:  0.1147192630982159 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.3  MSE:  0.11541920476802364 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.35  MSE:  0.11684121449366783 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.4  MSE:  0.11767535956289235 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.45  MSE:  0.11897452724741886 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.5  MSE:  0.12134795990502478 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.55  MSE:  0.12267397262529645 Accuracy:  0.90470183345785\n",
      "Fold: 3  Max Depth:  9  Learning Rate:  0.6  MSE:  0.12501592035097991 Accuracy:  0.90470183345785\n"
     ]
    }
   ],
   "source": [
    "# Intialize MSE and accuracy storage\n",
    "mses = [[0 for _ in range(4)] for _ in range(32)]\n",
    "accs = [[0 for _ in range(4)] for _ in range(32)]\n",
    "\n",
    "# Loop though folds\n",
    "for fold in range(0, len(X_trains_smote)):\n",
    "    # Set up data for the fold\n",
    "    fold_inds = [i for i in range(len(X_trains_smote)) if i != fold]\n",
    "    fold_dfs = [X_trains_smote[i] for i in fold_inds]\n",
    "    fold_ys = [y_trains_smote[i] for i in fold_inds]\n",
    "    all_fold_dfs = pd.concat(fold_dfs)\n",
    "    all_fold_ys = pd.concat(fold_ys)\n",
    "    count = 0\n",
    "    # Loop through possible max depths\n",
    "    for m_depth in [6,7,8,9]:\n",
    "        # Loop through possible learning rates\n",
    "        for lrate in [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:\n",
    "            # Fit classifier\n",
    "            xgb_clf_smote = xgb.XGBClassifier(learning_rate = lrate, max_depth = m_depth)\n",
    "            xgb_clf_smote.fit(all_fold_dfs, all_fold_ys)\n",
    "            # Make predictions\n",
    "            xgb_preds = xgb_clf_smote.predict_proba(limit_X_test)[:,1]\n",
    "            # Compute metrics\n",
    "            mses[count][fold] = np.square(np.subtract(y_test,xgb_preds)).mean()\n",
    "            accs[count][fold] = sum(xgb_clf.predict(limit_X_test_smote) == y_test)/len(y_test)\n",
    "            print(\"Fold:\", fold,\" Max Depth: \", m_depth, \" Learning Rate: \", lrate, \" MSE: \", mses[count][fold], \"Accuracy: \", accs[count][fold])\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c155cc9-5b2e-4638-873b-2133251414fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Choose hyperparameters that minimize average MSE across folds\n",
    "avg_mses = [0] * len(mses)\n",
    "for i in range(0,len(mses)):\n",
    "    avg_mses[i] = sum(mses[i])/len(mses[i])\n",
    "\n",
    "# Optimal is max depth = 7, lr = 0.4\n",
    "print(min(range(len(avg_mses)), key=avg_mses.__getitem__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d82bbbaa-9220-4408-9dc9-9d2be254d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xGBoost MSE:  0.1383792211943327\n",
      "xGBoost Accuracy:  0.8046883611556785\n",
      "Train xGBoost MSE:  0.08109596267549896\n",
      "Train xGBoost Accuracy:  0.8898787301423826\n"
     ]
    }
   ],
   "source": [
    "# Fit optimal xGBoost model and check performance on testing data\n",
    "opt_xgb_clf_smote = xgb.XGBClassifier(learning_rate = 0.4, max_depth = 7)\n",
    "opt_xgb_clf_smote.fit(limit_X_train_smote, y_train_smote)\n",
    "\n",
    "preds = opt_xgb_clf_smote.predict_proba(limit_X_test_smote)[:,1]\n",
    "print(\"xGBoost MSE: \", np.square(np.subtract(y_test,preds)).mean())\n",
    "print(\"xGBoost Accuracy: \", sum(opt_xgb_clf_smote.predict(limit_X_test_smote) == y_test)/len(y_test))\n",
    "\n",
    "# Check performance on training\n",
    "preds = opt_xgb_clf_smote.predict_proba(limit_X_train_smote)[:,1]\n",
    "print(\"Train xGBoost MSE: \", np.square(np.subtract(y_train_smote,preds)).mean())\n",
    "print(\"Train xGBoost Accuracy: \", sum(opt_xgb_clf_smote.predict(limit_X_train_smote) == y_train_smote)/len(y_train_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a56b40-cfb3-4985-a678-d7b17217aa96",
   "metadata": {},
   "source": [
    "## SMOTE Create two node graph data for graph neural network (GNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f526667d-fc30-44ca-aa9d-90d30146bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for two node graph generation\n",
    "class ShotDataset(Dataset):\n",
    "    def __init__(self, root, filename, transform=None, pre_transform=None, pre_filter=None):\n",
    "        \"\"\"\n",
    "        root: where the dataset should be stored, folder is split into raw_dir (downloaded dataset)\n",
    "        and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        super(ShotDataset, self).__init__(root, transform=None, pre_transform=None, pre_filter=None)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        If this file exists in raw_dir, the download is not triggered.\n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Not implemented\n",
    "        \"\"\"\n",
    "        return 'xxxxx.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, shot in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(shot)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(shot)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(shot)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(shot[\"goal\"])\n",
    "\n",
    "        \n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, edge_index=edge_index, edge_attr=edge_feats,y=label)\n",
    "            torch.save(data, os.path.join(self.processed_dir,f'shot_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, shot):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of the shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Initialize feature storage\n",
    "        all_node_feats = []\n",
    "        shooter_feats = []\n",
    "        goalie_feats = []\n",
    "\n",
    "        # Get shooter features\n",
    "        shooter_feats.append(shot[\"shooterName_target\"])\n",
    "        shooter_feats.append(shot[\"shotType_WRIST\"])\n",
    "        shooter_feats.append(shot[\"shotAngleReboundRoyalRoad\"])\n",
    "        all_node_feats.append(shooter_feats)\n",
    "\n",
    "        # Get goalie features\n",
    "        goalie_feats.append(shot[\"goalieNameForShot_target\"])\n",
    "        goalie_feats.append(shot[\"defendingTeamAverageTimeOnIce\"])\n",
    "        goalie_feats.append(shot[\"defendingTeamMaxTimeOnIceOfDefencemen\"])\n",
    "        all_node_feats.append(goalie_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, shot):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of ths shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Get edge features\n",
    "        all_edge_feats = []\n",
    "        edge_feats = []\n",
    "        edge_feats.append(shot[\"shotDistance\"])\n",
    "        all_edge_feats += [edge_feats, edge_feats]\n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, shot):\n",
    "        # Create adjacency matrix\n",
    "        edge_indices = []\n",
    "        edge_indices += [[0,1],[1,0]]\n",
    "        edge_indices = torch.tensor(edge_indices)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        # Get labels (goal or no goal)\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'shot_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "63655a48-9a0b-4d92-90ee-8a05b4f9a870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/vm4wbtfd3hz2hq37wm4d1_8h0000gp/T/ipykernel_3886/2847946480.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"goal\"] = y_test\n"
     ]
    }
   ],
   "source": [
    "# Put X and y back together for graph generation\n",
    "X_train_smote[\"goal\"] = y_train_smote\n",
    "X_test[\"goal\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b16af9b9-9f5a-4dca-a0a8-7b680996590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small subset for SMOTE two node graph generation\n",
    "X_train_smote.to_csv(\"COSC5P30/raw/train_dataset_smote.csv\", index = False)\n",
    "X_test.to_csv(\"COSC5P30/raw/test_dataset_smote.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a253e69-54a5-4441-a5b9-a93be1d8fecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████| 155356/155356 [00:47<00:00, 3290.66it/s]\n",
      "Done!\n",
      "Processing...\n",
      "100%|███████████████████████████████████| 81649/81649 [00:21<00:00, 3789.05it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create the graphs\n",
    "train_dat_smote = ShotDataset(filename = \"train_dataset_smote.csv\", root=\"COSC5P30/\")\n",
    "test_dat_smote = ShotDataset(filename = \"test_dataset_smote.csv\", root=\"COSC5P30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "37c562e4-ae6f-44be-b1c8-15eb1214e23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[ 0.0975,  1.0000,  0.0000],\n",
      "        [ 0.0973, 30.0000, 33.0000]]))\n",
      "('edge_index', tensor([[0, 1],\n",
      "        [1, 0]]))\n",
      "('edge_attr', tensor([[39.6989],\n",
      "        [39.6989]]))\n",
      "('y', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "# Check what the data looks like\n",
    "for p in train_dat_smote[0]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fd8bd3f0-5ff4-480e-b3fa-79caebc002fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ShotDataset(155356):\n",
      "====================\n",
      "Number of graphs: 155356\n",
      "Number of features: 3\n",
      "\n",
      "Data(x=[2, 3], edge_index=[2, 2], edge_attr=[2, 1], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 2\n",
      "Number of undirected edges: 1\n",
      "Average node degree: 1.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(f'Dataset: {train_dat_smote}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(train_dat_smote)}')\n",
    "print(f'Number of features: {train_dat_smote.num_features}')\n",
    "#print(f'Number of classes: {train_dat_smote.num_classes}')\n",
    "\n",
    "data = train_dat_smote[0]  # Get the first graph object\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of undirected edges: {data.num_edges // 2}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "59651d1a-bcc3-4eb2-a1ba-4ecd0883d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 155356\n",
      "Number of test graphs: 81649\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training graphs: {len(train_dat_smote)}')\n",
    "print(f'Number of test graphs: {len(test_dat_smote)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1818479-ab19-4d80-a5a2-0ecbff620a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "train_loader_smote = DataLoader(train_dat_smote, batch_size=1024, shuffle=True)\n",
    "test_loader_smote = DataLoader(test_dat_smote, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9d45e-a2b3-420b-98f1-e2bd657c04b2",
   "metadata": {},
   "source": [
    "## Fit two node SMOTE GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af67a1c6-2e70-4b44-b9fc-cbe291881e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create GCNclass GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(train_dat_smote.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels,1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return torch.sigmoid(self.lin(x))\n",
    "\n",
    "# choose either 64 or 128 hidden channels\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58aad6fd-54ca-4c6b-b394-36bea5141f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000000, Train Acc: 0.541543, Test Acc: 0.611410, Test MSE: 0.247975\n",
      "Epoch: 000001, Train Acc: 0.539297, Test Acc: 0.625960, Test MSE: 0.246215\n",
      "Epoch: 000002, Train Acc: 0.536786, Test Acc: 0.654607, Test MSE: 0.232960\n",
      "Epoch: 000003, Train Acc: 0.533941, Test Acc: 0.675893, Test MSE: 0.221482\n",
      "Epoch: 000004, Train Acc: 0.530543, Test Acc: 0.699188, Test MSE: 0.205572\n",
      "Epoch: 000005, Train Acc: 0.528618, Test Acc: 0.716273, Test MSE: 0.196426\n",
      "Epoch: 000006, Train Acc: 0.525818, Test Acc: 0.740560, Test MSE: 0.183655\n",
      "Epoch: 000007, Train Acc: 0.523424, Test Acc: 0.764345, Test MSE: 0.171649\n",
      "Epoch: 000008, Train Acc: 0.519858, Test Acc: 0.795833, Test MSE: 0.155739\n",
      "Epoch: 000009, Train Acc: 0.514090, Test Acc: 0.838663, Test MSE: 0.132843\n",
      "Epoch: 000010, Train Acc: 0.512185, Test Acc: 0.850825, Test MSE: 0.126639\n",
      "Epoch: 000011, Train Acc: 0.508670, Test Acc: 0.871560, Test MSE: 0.115424\n",
      "Epoch: 000012, Train Acc: 0.512275, Test Acc: 0.851768, Test MSE: 0.124811\n",
      "Epoch: 000013, Train Acc: 0.506443, Test Acc: 0.883048, Test MSE: 0.107196\n",
      "Epoch: 000014, Train Acc: 0.508059, Test Acc: 0.875161, Test MSE: 0.110857\n",
      "Epoch: 000015, Train Acc: 0.505536, Test Acc: 0.886612, Test MSE: 0.103269\n",
      "Epoch: 000016, Train Acc: 0.505806, Test Acc: 0.884653, Test MSE: 0.104692\n",
      "Epoch: 000017, Train Acc: 0.507402, Test Acc: 0.877941, Test MSE: 0.107305\n",
      "Epoch: 000018, Train Acc: 0.505523, Test Acc: 0.886686, Test MSE: 0.103613\n",
      "Epoch: 000019, Train Acc: 0.506752, Test Acc: 0.881787, Test MSE: 0.105880\n",
      "Epoch: 000020, Train Acc: 0.509662, Test Acc: 0.870605, Test MSE: 0.112908\n",
      "Epoch: 000021, Train Acc: 0.511264, Test Acc: 0.857683, Test MSE: 0.120772\n",
      "Epoch: 000022, Train Acc: 0.501905, Test Acc: 0.895884, Test MSE: 0.096680\n",
      "Epoch: 000023, Train Acc: 0.501139, Test Acc: 0.897157, Test MSE: 0.095177\n",
      "Epoch: 000024, Train Acc: 0.502105, Test Acc: 0.895479, Test MSE: 0.096547\n",
      "Epoch: 000025, Train Acc: 0.502877, Test Acc: 0.893789, Test MSE: 0.097655\n",
      "Epoch: 000026, Train Acc: 0.499363, Test Acc: 0.899227, Test MSE: 0.093613\n",
      "Epoch: 000027, Train Acc: 0.499305, Test Acc: 0.899264, Test MSE: 0.093243\n",
      "Epoch: 000028, Train Acc: 0.504770, Test Acc: 0.888192, Test MSE: 0.101938\n",
      "Epoch: 000029, Train Acc: 0.502961, Test Acc: 0.893348, Test MSE: 0.098578\n"
     ]
    }
   ],
   "source": [
    "# Set optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create training function\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data loader\n",
    "    for data in train_loader_smote:  \n",
    "        # Forward\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # Compute loss\n",
    "        loss = criterion(out, data.y.unsqueeze(1).float())\n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Create testing function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    sq_error = 0\n",
    "    correct = 0\n",
    "    goals = 0\n",
    "    # Iterate over data loader\n",
    "    for data in loader:\n",
    "        # Forward\n",
    "        out = model(data.x, data.edge_index, data.batch) \n",
    "        # Make prediction\n",
    "        pred = out\n",
    "        # Compute metrics\n",
    "        sq_error += (((pred - data.y.unsqueeze(1)))**2).sum()\n",
    "        correct += (torch.round(pred) == data.y.unsqueeze(1)).sum()\n",
    "        goals += sum(data.y.unsqueeze(1))\n",
    "     return sq_error / len(loader.dataset), correct / len(loader.dataset), pred, data.y, goals\n",
    "\n",
    "\n",
    "for epoch in range(0, 30):\n",
    "    train()\n",
    "    train_acc = test(train_loader_smote)\n",
    "    test_acc = test(test_loader_smote)\n",
    "    print(f'Epoch: {epoch:06d}, Train Acc: {train_acc[1].item():.6f}, Test Acc: {test_acc[1].item():.6f}, Test MSE: {test_acc[0].item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aadd7d-584a-40f1-85c7-e19079fc2244",
   "metadata": {},
   "source": [
    "## Create graph data for SMOTE graph neural network (GNN) with all players on the ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d344a22c-24fd-4e09-9846-6a6bed47a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for 12 node SMOTE graph generation\n",
    "class MultiPlayerShotDataset(Dataset):\n",
    "    def __init__(self, root, filename, transform=None, pre_transform=None, pre_filter=None):\n",
    "        \"\"\"\n",
    "        root: where the dataset should be stored, folder is split into raw_dir (downloaded dataset)\n",
    "        and processed_dir (processed data).\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        super(MultiPlayerShotDataset, self).__init__(root, transform=None, pre_transform=None, pre_filter=None)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \"\"\"\n",
    "        If this file exists in raw_dir, the download is not triggered.\n",
    "        \"\"\"\n",
    "        return self.filename\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\"\n",
    "        Not implemented\n",
    "        \"\"\"\n",
    "        return 'xxxxx.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.data = pd.read_csv(self.raw_paths[0])\n",
    "        for index, shot in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
    "            # Get number of players on the ice\n",
    "            n_forwards_shooting_team = int(shot[\"shootingTeamForwardsOnIce\"])\n",
    "            n_forwards_defending_team = int(shot[\"defendingTeamForwardsOnIce\"])\n",
    "            n_defencemen_shooting_team = int(shot[\"shootingTeamDefencemenOnIce\"])\n",
    "            n_defencemen_defending_team = int(shot[\"defendingTeamDefencemenOnIce\"])\n",
    "            n_players_on_ice = n_forwards_shooting_team + n_forwards_defending_team + n_defencemen_shooting_team + n_defencemen_defending_team\n",
    "            # Get node features\n",
    "            node_feats = self._get_node_features(shot, n_forwards_shooting_team, n_forwards_defending_team, n_defencemen_shooting_team, n_defencemen_defending_team)\n",
    "            # Get edge features\n",
    "            edge_feats = self._get_edge_features(shot, n_players_on_ice + 1)\n",
    "            # Get adjacency info\n",
    "            edge_index = self._get_adjacency_info(shot)\n",
    "            # Get labels info\n",
    "            label = self._get_labels(shot[\"goal\"])\n",
    "\n",
    "        \n",
    "            # Create data object\n",
    "            data = Data(x=node_feats, edge_index=edge_index.t().contiguous(), edge_attr=edge_feats,y=label)\n",
    "            torch.save(data, os.path.join(self.processed_dir,f'shot_{index}.pt'))\n",
    "\n",
    "    def _get_node_features(self, shot, n_forwards_shooting_team, n_forwards_defending_team, n_defencemen_shooting_team, n_defencemen_defending_team):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of the shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Get shooter and goalie features\n",
    "        all_node_feats = []\n",
    "        shooter_feats = [shot[\"shooterName_target\"], shot[\"shotType_WRIST\"], shot[\"shotAngleReboundRoyalRoad\"]]\n",
    "        goalie_feats = [shot[\"goalieNameForShot_target\"], shot[\"defendingTeamAverageTimeOnIce\"], shot[\"defendingTeamMaxTimeOnIceOfDefencemen\"]]\n",
    "        all_node_feats.append(shooter_feats)\n",
    "        all_node_feats.append(goalie_feats)\n",
    "\n",
    "        # Get features for all four adjacent player categories, checking if a defencemen took the shot\n",
    "        if shot[\"playerPositionThatDidEvent_D\"] == 1:\n",
    "            for i in range(n_defencemen_shooting_team - 1):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfDefencemen\"], shot[\"shootingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for j in range(n_forwards_shooting_team):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfForwards\"], shot[\"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for k in range(n_defencemen_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfDefencemen\"], shot[\"defendingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for l in range(n_forwards_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfForwards\"], shot[\"defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "        elif shot[\"playerPositionThatDidEvent_D\"] == 0:\n",
    "            for i in range(n_defencemen_shooting_team):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfDefencemen\"], shot[\"shootingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for j in range(n_forwards_shooting_team - 1):\n",
    "                adj_player_feats = [shot[\"shootingTeamAverageTimeOnIceOfForwards\"], shot[\"shootingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for k in range(n_defencemen_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfDefencemen\"], shot[\"defendingTeamAverageTimeOnIceOfDefencemenSinceFaceoff\"], 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "            for l in range(n_forwards_defending_team):\n",
    "                adj_player_feats = [shot[\"defendingTeamAverageTimeOnIceOfForwards\"], shot[\"defendingTeamAverageTimeOnIceOfForwardsSinceFaceoff\"], 1]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "\n",
    "        # Add padding if not maximum players\n",
    "        n_player_nodes = n_defencemen_shooting_team + n_forwards_shooting_team + n_defencemen_defending_team + n_forwards_defending_team\n",
    "        if n_player_nodes < 11:\n",
    "            for i in range(11 - n_player_nodes):\n",
    "                adj_player_feats = [0, 0, 0]\n",
    "                all_node_feats.append(adj_player_feats)\n",
    "\n",
    "        all_node_feats = np.asarray(all_node_feats)\n",
    "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_edge_features(self, shot, n_players_on_ice):\n",
    "        \"\"\"\n",
    "        This will return a matrix/2d array of ths shape [# of Nodes, Node Feature size]\n",
    "        \"\"\"\n",
    "        # Get edge feature for shooter and goalie\n",
    "        all_edge_feats = []\n",
    "        edge_feats = [shot[\"shotDistance\"]]\n",
    "        all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        # Get edge features for adjacent players\n",
    "        for i in range(0, n_players_on_ice):\n",
    "            for j in range(i+1, n_players_on_ice):\n",
    "                if i == 0 and j == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    random_distance = random.uniform(0, 75)\n",
    "                    edge_feats = [random_distance]\n",
    "                    all_edge_feats += [edge_feats, edge_feats]\n",
    "\n",
    "        # Add padding if not maximum players\n",
    "        if n_players_on_ice < 12:\n",
    "            for i in range(0, n_players_on_ice):\n",
    "                for j in range(n_players_on_ice, 12):\n",
    "                    edge_feats = [0]\n",
    "                    all_edge_feats += [edge_feats, edge_feats]\n",
    "                \n",
    "        all_edge_feats = np.asarray(all_edge_feats)\n",
    "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "    def _get_adjacency_info(self, shot):\n",
    "        # Create adjacency matrix\n",
    "        edge_indices = []\n",
    "\n",
    "        # There are 12 fully connected nodes\n",
    "        for i in range(0, 12):\n",
    "            for j in range(i+1, 12):\n",
    "                edge_indices.append([i, j])\n",
    "                edge_indices.append([j, i])\n",
    "        edge_indices = torch.tensor(edge_indices, dtype=torch.long)\n",
    "        return edge_indices\n",
    "\n",
    "    def _get_labels(self, label):\n",
    "        # Get labels (goal or no goal)\n",
    "        label = np.asarray([label])\n",
    "        return torch.tensor(label, dtype=torch.int64)\n",
    "\n",
    "    def len(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'shot_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "51feb3c3-00bf-4d0e-ab0b-04638da3493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small subset for SMOTE 12 node graph generation\n",
    "X_train_smote.to_csv(\"COSC5P30/raw/train_dataset_smote12.csv\", index = False)\n",
    "X_test.to_csv(\"COSC5P30/raw/test_dataset_smote12.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "578a3ea9-414f-49af-bfa0-e50b4646627a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "100%|█████████████████████████████████| 155356/155356 [01:03<00:00, 2449.05it/s]\n",
      "Done!\n",
      "Processing...\n",
      "100%|███████████████████████████████████| 81649/81649 [00:33<00:00, 2471.84it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Create the graphs\n",
    "train_dat_smote12 = MultiPlayerShotDataset(filename = \"train_dataset_smote12.csv\", root=\"COSC5P30/\")\n",
    "test_dat_smote12 = MultiPlayerShotDataset(filename = \"test_dataset_smote12.csv\", root=\"COSC5P30/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "07dfcd44-3e91-41e8-af00-a11ce6d09ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x', tensor([[ 0.0975,  1.0000,  0.0000],\n",
      "        [ 0.0973, 30.0000, 33.0000],\n",
      "        [20.5000, 20.5000,  0.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [25.6667, 24.0000,  1.0000],\n",
      "        [32.0000, 26.0000,  0.0000],\n",
      "        [32.0000, 26.0000,  0.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [28.6667, 26.0000,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]]))\n",
      "('edge_index', tensor([[ 0,  1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,\n",
      "          0, 10,  0, 11,  1,  2,  1,  3,  1,  4,  1,  5,  1,  6,  1,  7,  1,  8,\n",
      "          1,  9,  1, 10,  1, 11,  2,  3,  2,  4,  2,  5,  2,  6,  2,  7,  2,  8,\n",
      "          2,  9,  2, 10,  2, 11,  3,  4,  3,  5,  3,  6,  3,  7,  3,  8,  3,  9,\n",
      "          3, 10,  3, 11,  4,  5,  4,  6,  4,  7,  4,  8,  4,  9,  4, 10,  4, 11,\n",
      "          5,  6,  5,  7,  5,  8,  5,  9,  5, 10,  5, 11,  6,  7,  6,  8,  6,  9,\n",
      "          6, 10,  6, 11,  7,  8,  7,  9,  7, 10,  7, 11,  8,  9,  8, 10,  8, 11,\n",
      "          9, 10,  9, 11, 10, 11],\n",
      "        [ 1,  0,  2,  0,  3,  0,  4,  0,  5,  0,  6,  0,  7,  0,  8,  0,  9,  0,\n",
      "         10,  0, 11,  0,  2,  1,  3,  1,  4,  1,  5,  1,  6,  1,  7,  1,  8,  1,\n",
      "          9,  1, 10,  1, 11,  1,  3,  2,  4,  2,  5,  2,  6,  2,  7,  2,  8,  2,\n",
      "          9,  2, 10,  2, 11,  2,  4,  3,  5,  3,  6,  3,  7,  3,  8,  3,  9,  3,\n",
      "         10,  3, 11,  3,  5,  4,  6,  4,  7,  4,  8,  4,  9,  4, 10,  4, 11,  4,\n",
      "          6,  5,  7,  5,  8,  5,  9,  5, 10,  5, 11,  5,  7,  6,  8,  6,  9,  6,\n",
      "         10,  6, 11,  6,  8,  7,  9,  7, 10,  7, 11,  7,  9,  8, 10,  8, 11,  8,\n",
      "         10,  9, 11,  9, 11, 10]]))\n",
      "('edge_attr', tensor([[39.6989],\n",
      "        [39.6989],\n",
      "        [42.4304],\n",
      "        [42.4304],\n",
      "        [23.5447],\n",
      "        [23.5447],\n",
      "        [50.9418],\n",
      "        [50.9418],\n",
      "        [72.1147],\n",
      "        [72.1147],\n",
      "        [35.4992],\n",
      "        [35.4992],\n",
      "        [41.6966],\n",
      "        [41.6966],\n",
      "        [44.5610],\n",
      "        [44.5610],\n",
      "        [23.9899],\n",
      "        [23.9899],\n",
      "        [ 1.0656],\n",
      "        [ 1.0656],\n",
      "        [38.7903],\n",
      "        [38.7903],\n",
      "        [73.5760],\n",
      "        [73.5760],\n",
      "        [39.4755],\n",
      "        [39.4755],\n",
      "        [61.2157],\n",
      "        [61.2157],\n",
      "        [45.5720],\n",
      "        [45.5720],\n",
      "        [23.2351],\n",
      "        [23.2351],\n",
      "        [15.5589],\n",
      "        [15.5589],\n",
      "        [ 8.5054],\n",
      "        [ 8.5054],\n",
      "        [72.9218],\n",
      "        [72.9218],\n",
      "        [ 3.9408],\n",
      "        [ 3.9408],\n",
      "        [66.3738],\n",
      "        [66.3738],\n",
      "        [26.9116],\n",
      "        [26.9116],\n",
      "        [14.2645],\n",
      "        [14.2645],\n",
      "        [34.1144],\n",
      "        [34.1144],\n",
      "        [ 9.0336],\n",
      "        [ 9.0336],\n",
      "        [ 3.2205],\n",
      "        [ 3.2205],\n",
      "        [19.6432],\n",
      "        [19.6432],\n",
      "        [61.7241],\n",
      "        [61.7241],\n",
      "        [55.8441],\n",
      "        [55.8441],\n",
      "        [65.9315],\n",
      "        [65.9315],\n",
      "        [ 5.1589],\n",
      "        [ 5.1589],\n",
      "        [54.4040],\n",
      "        [54.4040],\n",
      "        [46.9641],\n",
      "        [46.9641],\n",
      "        [33.8029],\n",
      "        [33.8029],\n",
      "        [ 0.6104],\n",
      "        [ 0.6104],\n",
      "        [16.1791],\n",
      "        [16.1791],\n",
      "        [65.8493],\n",
      "        [65.8493],\n",
      "        [61.1135],\n",
      "        [61.1135],\n",
      "        [61.8426],\n",
      "        [61.8426],\n",
      "        [30.6924],\n",
      "        [30.6924],\n",
      "        [ 5.7867],\n",
      "        [ 5.7867],\n",
      "        [60.2016],\n",
      "        [60.2016],\n",
      "        [27.5315],\n",
      "        [27.5315],\n",
      "        [14.3780],\n",
      "        [14.3780],\n",
      "        [63.9661],\n",
      "        [63.9661],\n",
      "        [66.7507],\n",
      "        [66.7507],\n",
      "        [17.3193],\n",
      "        [17.3193],\n",
      "        [43.4490],\n",
      "        [43.4490],\n",
      "        [62.9174],\n",
      "        [62.9174],\n",
      "        [19.6241],\n",
      "        [19.6241],\n",
      "        [13.5220],\n",
      "        [13.5220],\n",
      "        [20.7898],\n",
      "        [20.7898],\n",
      "        [16.3773],\n",
      "        [16.3773],\n",
      "        [40.8238],\n",
      "        [40.8238],\n",
      "        [ 8.7904],\n",
      "        [ 8.7904],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000]]))\n",
      "('y', tensor([0]))\n"
     ]
    }
   ],
   "source": [
    "# Check what the data looks like\n",
    "for p in train_dat_smote12[0]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dba17931-cc15-4ab7-a48a-0ef8d8569ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MultiPlayerShotDataset(155356):\n",
      "====================\n",
      "Number of graphs: 155356\n",
      "Number of features: 3\n",
      "\n",
      "Data(x=[12, 3], edge_index=[2, 132], edge_attr=[132, 1], y=[1])\n",
      "=============================================================\n",
      "Number of nodes: 12\n",
      "Number of undirected edges: 66\n",
      "Average node degree: 11.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "# Print information about the dataset\n",
    "print(f'Dataset: {train_dat_smote12}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(train_dat_smote12)}')\n",
    "print(f'Number of features: {train_dat_smote12.num_features}')\n",
    "#print(f'Number of classes: {train_dat_adj.num_classes}')\n",
    "\n",
    "data = train_dat_smote12[0]  # Get the first graph object\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of undirected edges: {data.num_edges // 2}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3906cf17-66d1-4735-bcae-85de4a9486d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 155356\n",
      "Number of test graphs: 81649\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training graphs: {len(train_dat_smote12)}')\n",
    "print(f'Number of test graphs: {len(test_dat_smote12)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "801a4503-7ed0-4835-8168-999178d13486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat data loader\n",
    "train_loader_smote12 = DataLoader(train_dat_smote12, batch_size=1024, shuffle=True)\n",
    "test_loader_smote12 = DataLoader(test_dat_smote12, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5eaa5-8abe-4c9b-9d68-283eaaa0a3e1",
   "metadata": {},
   "source": [
    "## Fit SMOTE GNN with adjacent players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b461b268-f757-4998-946f-b673fe16295b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(3, 128)\n",
      "  (conv2): GCNConv(128, 128)\n",
      "  (conv3): GCNConv(128, 128)\n",
      "  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lin): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(train_dat_smote12.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels,1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  \n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return torch.sigmoid(self.lin(x))\n",
    "\n",
    "# Choose between 64 and 128 hidden channels\n",
    "model = GCN(hidden_channels=128)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7b924cc8-2752-4165-bf7f-390ca7064454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000000, Train Acc: 0.533671, Test Acc: 0.668667, Test MSE: 0.224576\n",
      "Epoch: 000001, Train Acc: 0.534900, Test Acc: 0.741748, Test MSE: 0.193008\n",
      "Epoch: 000002, Train Acc: 0.540597, Test Acc: 0.754976, Test MSE: 0.186255\n",
      "Epoch: 000003, Train Acc: 0.552422, Test Acc: 0.706549, Test MSE: 0.230756\n",
      "Epoch: 000004, Train Acc: 0.549345, Test Acc: 0.739507, Test MSE: 0.205387\n",
      "Epoch: 000005, Train Acc: 0.559399, Test Acc: 0.674656, Test MSE: 0.252732\n",
      "Epoch: 000006, Train Acc: 0.541736, Test Acc: 0.771093, Test MSE: 0.182785\n",
      "Epoch: 000007, Train Acc: 0.548250, Test Acc: 0.759409, Test MSE: 0.192915\n",
      "Epoch: 000008, Train Acc: 0.531334, Test Acc: 0.809759, Test MSE: 0.149540\n",
      "Epoch: 000009, Train Acc: 0.526945, Test Acc: 0.828841, Test MSE: 0.136673\n",
      "Epoch: 000010, Train Acc: 0.524177, Test Acc: 0.842827, Test MSE: 0.125913\n",
      "Epoch: 000011, Train Acc: 0.521911, Test Acc: 0.850372, Test MSE: 0.122494\n",
      "Epoch: 000012, Train Acc: 0.521596, Test Acc: 0.854499, Test MSE: 0.120571\n",
      "Epoch: 000013, Train Acc: 0.516620, Test Acc: 0.864787, Test MSE: 0.113160\n",
      "Epoch: 000014, Train Acc: 0.520173, Test Acc: 0.856055, Test MSE: 0.118246\n",
      "Epoch: 000015, Train Acc: 0.522149, Test Acc: 0.855479, Test MSE: 0.119193\n",
      "Epoch: 000016, Train Acc: 0.534855, Test Acc: 0.815834, Test MSE: 0.143916\n",
      "Epoch: 000017, Train Acc: 0.530060, Test Acc: 0.835540, Test MSE: 0.130669\n",
      "Epoch: 000018, Train Acc: 0.505220, Test Acc: 0.891046, Test MSE: 0.102630\n",
      "Epoch: 000019, Train Acc: 0.533414, Test Acc: 0.824113, Test MSE: 0.136588\n",
      "Epoch: 000020, Train Acc: 0.524750, Test Acc: 0.853238, Test MSE: 0.120700\n",
      "Epoch: 000021, Train Acc: 0.510022, Test Acc: 0.884065, Test MSE: 0.105990\n",
      "Epoch: 000022, Train Acc: 0.503527, Test Acc: 0.895859, Test MSE: 0.102146\n",
      "Epoch: 000023, Train Acc: 0.537598, Test Acc: 0.820769, Test MSE: 0.139515\n",
      "Epoch: 000024, Train Acc: 0.519986, Test Acc: 0.868584, Test MSE: 0.112008\n",
      "Epoch: 000025, Train Acc: 0.533993, Test Acc: 0.834732, Test MSE: 0.129598\n",
      "Epoch: 000026, Train Acc: 0.526925, Test Acc: 0.858700, Test MSE: 0.119427\n",
      "Epoch: 000027, Train Acc: 0.517553, Test Acc: 0.873593, Test MSE: 0.111334\n",
      "Epoch: 000028, Train Acc: 0.533304, Test Acc: 0.839998, Test MSE: 0.129358\n",
      "Epoch: 000029, Train Acc: 0.525065, Test Acc: 0.859790, Test MSE: 0.116599\n"
     ]
    }
   ],
   "source": [
    "# Set optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create training function\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over data loader\n",
    "    for data in train_loader_smote12:\n",
    "        # Forward\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        # Loss computation\n",
    "        loss = criterion(out, data.y.unsqueeze(1).float())\n",
    "        # Gradient\n",
    "        loss.backward()\n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Create testing function\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    sq_error = 0\n",
    "    correct = 0\n",
    "    goals = 0\n",
    "    # Iterate over data loader\n",
    "    for data in loader:\n",
    "        # Forward\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        # Make predictions\n",
    "        pred = out\n",
    "        # Compute metrics\n",
    "        sq_error += (((pred - data.y.unsqueeze(1)))**2).sum()\n",
    "        correct += (torch.round(pred) == data.y.unsqueeze(1)).sum()\n",
    "        goals += sum(data.y.unsqueeze(1))\n",
    "    return sq_error / len(loader.dataset), correct / len(loader.dataset), pred, data.y, goals\n",
    "\n",
    "\n",
    "for epoch in range(0, 30):\n",
    "    train()\n",
    "    train_acc = test(train_loader_smote12)\n",
    "    test_acc = test(test_loader_smote12)\n",
    "    print(f'Epoch: {epoch:06d}, Train Acc: {train_acc[1].item():.6f}, Test Acc: {test_acc[1].item():.6f}, Test MSE: {test_acc[0].item():.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
